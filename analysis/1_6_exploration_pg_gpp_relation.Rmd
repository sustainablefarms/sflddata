---
title: "Very Simple Models for GPP"
author: "Kassel Hingee"
date: "24/01/2020"
output: html_document
---

## Preparation
```{r setup, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE)
out <- lapply(c("sf", "tsibble", 'lubridate', "viridis",
                'ggplot2', 'tidyr', 'grid', 'gridExtra', 
                'feasts', 'dplyr', 'gtable', 'fable',
                'mgcv'),
       library, character.only = TRUE)
out <- lapply(paste0("../functions/", list.files("../functions/")), source)
```

```{r preparedata}
#load data and convert to tsibbles
load("../private/data/remote_sensed/pg_daily.Rdata")
pg_daily$times <- as_date(pg_daily$times)
pg <- pg_daily %>%
  pivot_longer(-times, names_to = "site", values_to = "pg") %>%
  as_tsibble(key = site, index = times)
load("../private/data/remote_sensed/gpp_8d.Rdata")
gpp <- gpp_8d %>%
  pivot_longer(-times, names_to = "site", values_to = "gpp") %>%
  as_tsibble(key = site, index = times)
pggpp <- as_tsibble(dplyr::full_join(pg, gpp, by = c("times", "site")), key = site, index = times)

# add times breakdowns
pggpp <- pggpp %>%
  mutate(yday = yday(times),
         year = year(times))
  

#interpolate gpp
pggpp <- pggpp %>%
  group_by_key() %>% #key is site
  mutate(lininterp_gpp = zoo::na.approx(gpp, rule = 2, na.rm = FALSE)) %>% #rule 2 means edges are assigned last value
  ungroup()

#make sure sites are ordered alphabetically
pggpp <- pggpp %>%
  arrange(site) %>%
  mutate(site = factor(site, ordered = TRUE))

#separate alpha part of site code
pggpp <- pggpp %>%
  mutate(farm = factor(substr(site, 1, 4)),
         sitenum = factor(as.integer(substr(site, 5, 5))))


# Add cumulative rainfalls
pggpp <- pggpp %>% 
  group_by_key() %>%
  mutate(pg_cumsum = cumsum(pg)) %>%
  mutate(pg_1to7 = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 8), # 1 up to 8 days behind (excluding 8th day)
         pg_1to15 = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 16), # 1 to 16 days behind
         pg_1to1m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 31),
         pg_1to2m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 2*31),
         pg_1to3m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 3*31),
         pg_1to4m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 4*31),
         pg_1to5m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 5*31),
         pg_1to6m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 6*31),
         pg_1to7m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 7*31),
         pg_1to8m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 8*31),
         pg_1to10m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 10*31),
         pg_1to12m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 12*31),
         pg_1to14m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 14*31),
         ) %>%
  mutate(pg_8d = pg_cumsum - lag(pg_cumsum, n = 8),
         #0 to day 7 (8 days) cumulative rainfall to correspond with gaps in GPP
         pg_24d = pg_cumsum - lag(pg_cumsum, n = 3*8)) %>% 
         # every 24 days (3*8) of GPP should be less correlated (given average GPP), this pg_24d corresponds to the rain through that period
  ungroup()
```

```{r seasonalreferences}
# simple median of values
ydaymedian <- pggpp %>%
  filter(yday %in% seq(1, 366, by = 8)) %>%
  group_by(site) %>%
  index_by(yday) %>%
  summarise(gpp.ydaymed = median(gpp),
            pg_8d.ydaymed = median(pg_8d),
            pg_24d.ydaymed = median(pg_24d),
            pg_1to5m.ydaymed = median(pg_1to5m, na.rm = TRUE))
pggpp <- left_join(pggpp, ydaymedian, by = c("site", "yday"))
```

```{r preparetraintest}
train <- pggpp %>%
  filter(yday %in% seq(1, 366, by = 8)) %>%
  filter(sitenum == 1) %>% #so not doubling up at farms
  filter_index(. ~ "2016-12-31") %>%
  select(-pg_cumsum, -lininterp_gpp) %>%
  mutate(gpp = if_else(gpp < 0.1, as.double(NA), gpp)) #remove outlying GPP values that are super low

test <- pggpp %>%
  filter_index("2017-01-01" ~ .) %>%
  filter(sitenum == 1) %>% #so not doubling up at farms
  filter(yday %in% seq(1, 366, by = 8)) %>%
  select(-pg_cumsum)
```


In the above very small GPP values have been removed. There was `r sum(is.na(train$gpp))` of them, which corresponds to `r sum(is.na(train$gpp)) / length(train$gpp)` of the data.

## Linear Model of GPP
### Single order with interaction
Recipricoal of pg_1to5m.ydaymed included.
```{r m1}
filtertrain <- function(x){
  x <- x %>% filter(sitenum == 1,
                    #farm %in% c("BELL", "WILS", "GEDD"),
                    !is.na(pg_1to14m),
                    yday %in% seq(1, 366, by = 3 * 8), #reduce correlation in data fitting and reduce fitting time
                    !is.na(gpp)) # %>%
  #   mutate(recippg_1to5m.ydaymed = 1/pg_1to5m.ydaymed)
  # x$recippg_1to5m.ydaymed <- scale(x$recippg_1to5m.ydaymed)
  return(x)
}

m1 <- lm(box_cox(gpp / gpp.ydaymed, lambda = 0.2626) ~ 
              pg_1to5m * I(1 / pg_1to5m.ydaymed) * pg_24d,
              data = train %>% filtertrain() )

plot(m1)
summary(m1)

pred <- train %>%
  filtertrain() %>%
  tibble::rownames_to_column(var = "rowname") %>%
  left_join(as_tibble(x = data.frame(predict(m1, type = "response", se.fit = TRUE)[1:2]), rownames = "rowname"),
            by = "rowname") %>% 
  mutate(pred_m1 = inv_box_cox(fit, lambda = 0.2626) * gpp.ydaymed) %>%
  mutate(pred_m1_upper = inv_box_cox(fit + 2 * se.fit, lambda = 0.2626) * gpp.ydaymed) %>%
  mutate(pred_m1_lower = inv_box_cox(fit - 2 * se.fit, lambda = 0.2626) * gpp.ydaymed) 
pred %>%
  filter(farm %in% c("WILS", "GEDD", "MATH", "SMAR")) %>%
  ggplot() +
  facet_wrap(~farm) +
  geom_ribbon(aes(x = times, ymin = pred_m1_lower, ymax = pred_m1_upper)) +
  geom_line(aes(x = times, y = pred_m1)) +
  geom_line(aes(x = times, y = gpp.ydaymed), col = "grey", linetype = "dashed", size = 0.5) +
  geom_line(aes(x = times, y = gpp), col = "blue") +
  #geom_point(aes(x = times, y = value, col = name, shape = name)) +
  ggtitle("m1")
```

Minus a few outlying residuals, the distribution looks roughly constant in variance! The residuals are a long way off Gaussian at the larger end.
The adjusted R2 value of 0.417 is better than that of m11 in 1_5...Rmd.

But it lacks some physical sense: large pg_1to5m will have a negative (cancelling) effect.

I'm surprised that a linear model works so well. The stats are similar to the gams!

The confidence intervals are waaay too small. Suggesting a lot more correlation in the data than the computer knows about.

#### Investigation of residuals
```{r m1_residuals}
meanresid <- pred %>%
  mutate(resid = gpp - pred_m1) %>%
  as_tibble() %>%
  group_by(site) %>%
  summarise(avresid = mean(resid, na.rm = TRUE))
pred %>%
  mutate(resid = gpp - pred_m1) %>% 
  inner_join(meanresid, by = "site") %>%
  ggplot() +
  facet_wrap(~farm) +
  geom_line(aes(x = times, y = resid, col = avresid)) +
  scale_color_viridis_c() +
  #geom_point(aes(x = times, y = value, col = name, shape = name)) +
  ggtitle("m1 residuals") +
  scale_y_continuous(name = "observed - predicted gpp")
```

In the above, negative values mean the gpp prediction was higher than the observed gpp.
That GEDD has a highly negative average residual means that on average observed GPP was lower than what was predicted. *Even though GEDD usually has a high GPP, the prediction of GEDD's GPP is even higher!*

##### Spatial Distribution of Average of Site Residuals
```{r m1_spatialdistribution_avresiduals}
library(ggrepel)
sws_sites <- sws_sites_2_sf(readRDS("../private/data/clean/sws_sites.rds")) %>%
  mutate(latitude =  sf::st_coordinates(geometry)[, "Y"],
         longitude = sf::st_coordinates(geometry)[, "X"]) 

meanresid %>%
  left_join(sws_sites, by = c(site = "SiteCode")) %>%
  ggplot() +
  geom_point(aes(x = longitude, y = latitude, col = avresid, size = avresid)) +
  scale_color_viridis_c() +
  geom_text_repel(aes(x = longitude, y = latitude, label = site)) +
  guides(
    color = guide_colourbar("Average\nResidual Color"),
    size = guide_legend("Average\nResidual Size")
  ) +
  coord_fixed() +
  ggtitle("Average Residual of m1 for each site")
```

#### Does the inclusion of the reciprical median rainfal improve the model?
If the distributional assumptions could be trusted then the $p$ values from an anova test could be used to decide this. Looking at relative size of the $p$ values it seems the reciprical by itself is not very useful, but interacting with pg_1to5m is quite useful. Testing with a scaled centred reciprical gave the same results.

In liu of the mathematically sound method, lets try investigating a model without the reciprical.
```{r m1_b}
filtertrain <- function(x){
  x <- x %>% filter(sitenum == 1,
                    #farm %in% c("BELL", "WILS", "GEDD"),
                    !is.na(pg_1to14m),
                    yday %in% seq(1, 366, by = 3 * 8), #reduce correlation in data fitting and reduce fitting time
                    !is.na(gpp))
  return(x)
}

m1_b <- lm(box_cox(gpp / gpp.ydaymed, lambda = 0.2626) ~ 
              pg_1to5m * pg_24d,
              data = train %>% filtertrain() )

plot(m1_b)
summary(m1_b)

pred <- train %>%
  filtertrain() %>%
  tibble::rownames_to_column(var = "rowname") %>%
  left_join(as_tibble(x = data.frame(predict(m1_b, type = "response", se.fit = TRUE)[1:2]), rownames = "rowname"),
            by = "rowname") %>% 
  mutate(pred_m1_b = inv_box_cox(fit, lambda = 0.2626) * gpp.ydaymed) %>%
  mutate(pred_m1_b_upper = inv_box_cox(fit + 2 * se.fit, lambda = 0.2626) * gpp.ydaymed) %>%
  mutate(pred_m1_b_lower = inv_box_cox(fit - 2 * se.fit, lambda = 0.2626) * gpp.ydaymed) 
pred %>%
  filter(farm %in% c("WILS", "GEDD", "MATH", "SMAR")) %>%
  ggplot() +
  facet_wrap(~farm) +
  geom_ribbon(aes(x = times, ymin = pred_m1_b_lower, ymax = pred_m1_b_upper)) +
  geom_line(aes(x = times, y = pred_m1_b)) +
  geom_line(aes(x = times, y = gpp.ydaymed), col = "grey", linetype = "dashed", size = 0.5) +
  geom_line(aes(x = times, y = gpp), col = "blue") +
  #geom_point(aes(x = times, y = value, col = name, shape = name)) +
  ggtitle("m1 without recipricol of median precipitation")
```

The R2 value of this model is nearly identical to that of m1.
The predictions themselves also look ver similar.

### Full 2nd Order Polynomial Model
```{r m2}
filtertrain <- function(x){
  x <- x %>% filter(sitenum == 1,
                    #farm %in% c("BELL", "WILS", "GEDD"),
                    !is.na(pg_1to14m),
                    yday %in% seq(1, 366, by = 3 * 8), #reduce correlation in data fitting and reduce fitting time
                    !is.na(gpp)) 
  return(x)
}

m2 <- lm(box_cox(gpp / gpp.ydaymed, lambda = 0.2626) ~ 
              polym(pg_1to5m, I(1/pg_1to5m.ydaymed), pg_24d, degree = 2),
              data = train %>% filtertrain() )

plot(m2)
summary(m2)

pred <- train %>%
  filtertrain() %>%
  tibble::rownames_to_column(var = "rowname") %>%
  left_join(tibble::enframe(predict(m2, type = "response"), name = "rowname", value = "linpred_m2"),
            by = "rowname") %>% 
  mutate(pred_m2 = inv_box_cox(linpred_m2, lambda = 0.2626) * gpp.ydaymed) 
pred %>%
  filter(farm == "WILS") %>%
  pivot_longer(c(pred_m2, gpp)) %>%
  ggplot() +
  geom_line(aes(x = times, y = gpp.ydaymed), col = "black", size = 0.5) +
  geom_line(aes(x = times, y = value, col = name, lty = name)) +
  #geom_point(aes(x = times, y = value, col = name, shape = name)) +
  ggtitle("m2")
```

This model with higher orders has very similar R-squared value to m1, residuals etc, and all standard diagnostic plots look very similar.

### Full 5th Order Polynomial
```{r m3}
filtertrain <- function(x){
  x <- x %>% filter(sitenum == 1,
                    #farm %in% c("BELL", "WILS", "GEDD"),
                    !is.na(pg_1to14m),
                    yday %in% seq(1, 366, by = 3 * 8), #reduce correlation in data fitting and reduce fitting time
                    !is.na(gpp))
  return(x)
}

m3 <- lm(box_cox(gpp / gpp.ydaymed, lambda = 0.2626) ~ 
              polym(pg_1to5m, I(1/pg_1to5m.ydaymed), pg_24d, degree = 5),
              data = train %>% filtertrain() )

plot(m3)
summary(m3)

pred <- train %>%
  filtertrain() %>%
  tibble::rownames_to_column(var = "rowname") %>%
  left_join(tibble::enframe(predict(m3, type = "response"), name = "rowname", value = "linpred_m3"),
            by = "rowname") %>% 
  mutate(pred_m3 = inv_box_cox(linpred_m3, lambda = 0.2626) * gpp.ydaymed) 
pred %>%
  filter(farm == "WILS") %>%
  pivot_longer(c(pred_m3, gpp)) %>%
  ggplot() +
  geom_line(aes(x = times, y = gpp.ydaymed), col = "black", size = 0.5) +
  geom_line(aes(x = times, y = value, col = name, lty = name)) +
  #geom_point(aes(x = times, y = value, col = name, shape = name)) +
  ggtitle("m3")
```



### Single-order polynomial with interaction and site-specific effects
```{r m4}
filtertrain <- function(x){
  x <- x %>% filter(sitenum == 1,
                    #farm %in% c("BELL", "WILS", "GEDD"),
                    !is.na(pg_1to14m),
                    yday %in% seq(1, 366, by = 3 * 8), #reduce correlation in data fitting and reduce fitting time
                    !is.na(gpp))
  return(x)
}

m4 <- lm(box_cox(gpp / gpp.ydaymed, lambda = 0.2626) ~ 
              pg_1to5m * I(1 /  pg_1to5m.ydaymed) * pg_24d * farm,
              data = train %>% filtertrain() )

plot(m4)
summary(m4)

pred <- train %>%
  filtertrain() %>%
  tibble::rownames_to_column(var = "rowname") %>%
  left_join(tibble::enframe(predict(m4, type = "response"), name = "rowname", value = "linpred_m4"),
            by = "rowname") %>% 
  mutate(pred_m4 = inv_box_cox(linpred_m4, lambda = 0.2626) * gpp.ydaymed) 
pred %>%
  filter(farm == "WILS") %>%
  pivot_longer(c(pred_m4, gpp)) %>%
  ggplot() +
  geom_line(aes(x = times, y = gpp.ydaymed), col = "black", size = 0.5) +
  geom_line(aes(x = times, y = value, col = name, lty = name)) +
  #geom_point(aes(x = times, y = value, col = name, shape = name)) +
  ggtitle("m4")
```


Interesting that this model appears to do as well as the single order with interactions and no site effects model (model m1). 

## GAM of Cumulative precipitations (to double check) with identity link
In other documents I've nearly always fitted gams using log link. Given that transformation isn't required for the above models to perform well, I'm going to try it here without the log link.

```{r m5_gam}
filtertrain <- function(x){
  x <- x %>% filter(sitenum == 1,
                    #farm %in% c("BELL", "WILS", "GEDD"),
                    !is.na(pg_1to14m),
                    yday %in% seq(1, 366, by = 3 * 8), #reduce correlation in data fitting and reduce fitting time
                    !is.na(gpp))
  return(x)
}
  
m5 <- bam(box_cox(gpp / gpp.ydaymed, lambda = 0.2626) ~ 
            s(pg_24d) +
            s(pg_1to5m) +
            s(pg_1to5m.ydaymed) +
             ti(pg_24d, pg_1to5m) +
             ti(pg_1to5m, pg_1to5m.ydaymed) +
             ti(pg_24d, pg_1to5m.ydaymed) +
             ti(pg_24d, pg_1to5m,  pg_1to5m.ydaymed),
             data = train %>% filtertrain() )

gam.check(m5)
plot(m5)
summary(m5)

pred <- train %>%
  filtertrain() %>%
  tibble::rownames_to_column(var = "rowname") %>%
  left_join(as_tibble(x = data.frame(predict(m5, type = "response", se.fit = TRUE)[1:2]), rownames = "rowname"),
            by = "rowname") %>% 
  mutate(pred = inv_box_cox(fit, lambda = 0.2626) * gpp.ydaymed) %>%
  mutate(pred_upper = inv_box_cox(fit + 2 * se.fit, lambda = 0.2626) * gpp.ydaymed) %>%
  mutate(pred_lower = inv_box_cox(fit - 2 * se.fit, lambda = 0.2626) * gpp.ydaymed) 
pred %>%
  filter(farm %in% c("WILS", "GEDD", "MATH", "SMAR")) %>%
  ggplot() +
  facet_wrap(~farm) +
  geom_ribbon(aes(x = times, ymin = pred_lower, ymax = pred_upper)) +
  geom_line(aes(x = times, y = pred)) +
  geom_line(aes(x = times, y = gpp.ydaymed), col = "grey", linetype = "dashed", size = 0.5) +
  geom_line(aes(x = times, y = gpp), col = "blue") +
  #geom_point(aes(x = times, y = value, col = name, shape = name)) +
  ggtitle("m5")
```

The map of response and fitted values is much poorer than the linear models: a trend in the fitted values is clear, it is very strange that such a trend is present as it looks like a spline would easily remove such issues.

The predictions from m5 appear so similar to the prediction from m1, but there are small differences. GEDD predictions are substantially higher (perhaps because GEDD is an outlier).
The R2 value of m5 is slightly higher than the R2 value of m1.


## A linear regression with ARIMA errors to get more realistic prediction intervals
```{r arimaforerror_explore}
pred <- train %>%
  filtertrain() %>%
  tibble::rownames_to_column(var = "rowname") %>%
  left_join(as_tibble(x = data.frame(predict(m1, type = "response", se.fit = TRUE)[1:2]), rownames = "rowname"),
            by = "rowname") %>% 
  left_join(as_tibble(x = data.frame(linresid = residuals(m1, type = "response")), rownames = "rowname"),
            by = "rowname") %>% 
  mutate(pred_m1 = inv_box_cox(fit, lambda = 0.2626) * gpp.ydaymed) 

per <- length(seq(1, 366, by = 3 * 8))

pred %>%
  group_by_key() %>%
  select(linresid) %>%
  filter(site %in% c("BELL1", "WILS1")) %>%
  mutate(sd1 = difference(linresid, lag = 16)) %>%
  mutate(sd2 = difference(linresid, lag = 2 * 16)) %>%
  mutate(sd3 = difference(linresid, lag = 3 * 16)) %>%
  mutate(sd1_2 = difference(linresid, lag = 1 * 16, differences = 2)) %>%
  mutate(sd1_3 = difference(linresid, lag = 1 * 16, differences = 3)) %>%
  mutate(sd1_4 = difference(linresid, lag = 1 * 16, differences = 4)) %>%
  mutate(sd2_3 = difference(linresid, lag = 2 * 16, differences = 3)) %>%
  mutate(sd3_2 = difference(linresid, lag = 3 * 16, differences = 2)) %>%
  mutate(sd3_3 = difference(linresid, lag = 3 * 16, differences = 3)) %>%
  mutate(od1_sd1 = difference(sd1, lag = 1)) %>%
  mutate(od1_sd3 = difference(sd1, lag = 3)) %>%
  mutate(od2_sd1 = difference(sd1, lag = 2)) %>%
  mutate(od1_2_sd1 = difference(sd1, lag = 1, differences = 2)) %>%
  mutate(od1_sd3 = difference(sd3, lag = 1, differences = 1)) %>%
  mutate(od1_sd3_2 = difference(sd3_2, lag = 1, differences = 1)) %>%
  mutate(od1_sd3_3 = difference(sd3_2, lag = 1, differences = 3)) %>%
  mutate(od2_sd3 = difference(sd3, lag = 2, differences = 1)) %>%
  mutate(od1_2_sd3 = difference(sd3, lag = 1, differences = 2)) %>%
  pivot_longer(c(-times, -site)) %>%
  ggplot() +
  facet_grid(cols = vars(site), rows = vars(name), scales = "free_y") +
  geom_hline(yintercept = 0, col = "grey", lty = "dashed") +
  geom_line(aes(x = times, y = value)) +
  scale_y_continuous(name = "log(gpp + 1)") +
  ggtitle("Linear Residual: Seasonal Differences (sd) First")

```

Looks like the stationarity of the linear residuals isn't too bad. But differencing may help regardless.

Lets just try fitting something, maybe it will work this time!?
```{r m6_fitarima}
filtertrain <- function(x){
  x <- x %>% filter(sitenum == 1,
                    #farm %in% c("BELL", "WILS", "GEDD"),
                    !is.na(pg_1to14m),
                    yday %in% seq(1, 366, by = 3 * 8), #reduce correlation in data fitting and reduce fitting time
                    !is.na(gpp))
  return(x)
}
m6_sp <- ARIMA(box_cox(gpp / gpp.ydaymed, lambda = 0.2626) ~ 
              pg_1to5m * I(1 / pg_1to5m.ydaymed) * pg_24d +
              pdq())


m6 <- train %>% filtertrain() %>%
  tibble::rowid_to_column() %>% #reindex by approximate 8 day intervals (discrepancies at the end of every year)
  as_tsibble(key = NULL, index = "rowid") %>%
  ungroup() %>%
  model(m6_sp)

print(m6[[1, 1]])
tidy(m6[[1, 1]])

train %>% filtertrain() %>%
  tibble::rowid_to_column() %>%
  left_join(fitted(m6), by = "rowid") %>%
  filter(farm %in% c("WILS", "GEDD", "MATH", "SMAR")) %>%
  mutate(pred = .fitted * gpp.ydaymed) %>%
  ggplot() +
  facet_wrap(~farm) +
  geom_line(aes(x = times, y = pred)) +
  geom_line(aes(x = times, y = gpp.ydaymed), col = "grey", linetype = "dashed", size = 0.5) +
  geom_line(aes(x = times, y = gpp), col = "blue") +
  #geom_point(aes(x = times, y = value, col = name, shape = name)) +
  ggtitle("m6")
```


```{r forecastingwithout_pastgpp}
m6 %>%
  forecast(new_data = train %>% filtertrain() %>%
             #select(-gpp) %>% # I've checked that it doesn't use gpp
             tibble::rowid_to_column() %>%
             #filter(farm %in% c("GEDD")) %>% 
             update_tsibble(key = NULL, index = "rowid") %>%
             fill_gaps()
           ) %>%
  filter(!is.na(times)) %>%
  update_tsibble(key = "site", index = "times") %>%
  filter(farm %in% c("WILS", "GEDD", "MATH", "SMAR")) %>% 
  mutate(interval = hilo(.distribution, 95)) %>%
  ggplot() +
  facet_wrap(~farm, scales = "free_y") +
  geom_ribbon(aes(x = times, ymin = interval$.lower * gpp.ydaymed, ymax = interval$.upper * gpp.ydaymed), fill = "lightgrey") +
  geom_line(aes(x = times, y = `gpp/gpp.ydaymed` * gpp.ydaymed), col = "black") + 
  geom_line(aes(x = times, y = gpp), col = "blue") +
  ggtitle("m6")
```

Finally fitted a model where the error bars look believable! :). Probably worth checking autocorrelations of residuals though!

Prediction of mean is very close to what m1 was, which makes sense if one assumes there is enough mixing in the time series.

### Model Checking (preliminary)

#### Plotting Residuals
```{r m6_res_view}
preds <- m6[[1, 1]] %>%
  forecast(new_data = train %>% filtertrain() %>%
             #select(-gpp) %>% # I've checked that it doesn't use gpp
             tibble::rowid_to_column() %>%
             #filter(farm %in% c("GEDD")) %>% 
             update_tsibble(key = NULL, index = "rowid") %>%
             fill_gaps()
           ) %>%
  as_tsibble() %>% # the result of forecast is a special class called fable (fbl_ts) which requires response and distribution column to be baked in with unchanged names, so removing that extra structure
  rename(
    "gpp/gpp.ydaymed.pred" = `gpp/gpp.ydaymed`,
    "gpp/gpp.ydaymed.pred.dist" = `.distribution`  #The help for forecast in fabletools make it sound like this distribution is related to the backtransformed variable, which I think is impressive!
  ) %>%
  filter(!is.na(times)) %>%
  update_tsibble(key = "site", index = "times") %>%
  mutate(gpp.pred = `gpp/gpp.ydaymed.pred` * gpp.ydaymed) %>%
  mutate(lin.pred = box_cox(`gpp/gpp.ydaymed.pred`, lambda = 0.2626)) %>%
  mutate(lin.resid =  box_cox(gpp / gpp.ydaymed, lambda = 0.2626) - box_cox(`gpp/gpp.ydaymed.pred`, lambda = 0.2626)) %>%
  mutate(`gpp/gpp.ydaymed` =  gpp  / gpp.ydaymed) %>%
  mutate(lin.obs = box_cox( gpp / gpp.ydaymed, lambda = 0.2626))

preds %>%
  #filter(farm == "CAIN") %>%
  ggplot() +
  facet_wrap(~year) +
  geom_line(aes(x = yday, y = lin.resid, col = farm, group = farm)) +
  scale_color_viridis_d()
```



```{r resid_types}
residuals(m6[[1, 1]], type = "regression") %>%
  left_join(residuals(m6[[1, 1]], type = "innovation"), by = "rowid", suffix = c(".regression", ".innovation")) %>%
  left_join(preds, by = "rowid") %>%
  update_tsibble(key = "site", index = "times") %>%
  as_tibble() %>%
  select(lin.obs, .resid.regression)

# check what residuals.ARIMA() returns for type = "regression" or "innovation"
residuals(m6[[1, 1]], type = "regression") %>%
  left_join(residuals(m6[[1, 1]], type = "innovation"), by = "rowid", suffix = c(".regression", ".innovation")) %>%
  left_join(preds, by = "rowid") %>%
  update_tsibble(key = "site", index = "times") %>%
  as_tibble() %>%
  select(rowid, times, farm, 
         lin.pred,
         lin.obs,
         lin.resid,
         .resid.regression,
         .resid.innovation
         ) %>%
  #mutate(innov.p.linpred = lin.pred + cumsum(.resid.innovation))
  filter(farm %in% c("CAIN")) %>% 
  mutate(lin.pred.innov = lin.pred - lag(lin.pred, 1)) %>%
  mutate(lin.obs.innov = lin.obs - lag(lin.obs, 1)) %>%
  mutate(man.resid.innov = lin.obs.innov - lin.pred.innov) %>%
  pivot_longer(c(
    lin.pred.innov,
    lin.obs.innov,
    man.resid.innov
    )) %>%
  ggplot() +
  #facet_wrap(~farm, scales = "free_y") +
  facet_grid(rows = vars(name)) +
  geom_line(aes(x = times, y = .resid.innovation), col = "black", lty = "dotted") +
  geom_line(aes(x = times, y = value, col = name, lty = name)) 
```

It looks like `.resid.regression` is identical to the transformed observed value!
The '.resid.innovation' is very similar to the residual in the rate of change, but not quite the same.


#### Residual Frequency
Residuals are not Gaussian! The QQ plot has significant non-Gaussian tails, there are also a number of outliers. See below.

```{r m6_resid_frequencies}
residuals(m6) %>%
  ggplot() +
  geom_qq(aes(sample = .resid)) +
  stat_qq_line(aes(sample = .resid))

residuals(m6) %>%
  ggplot() +
  geom_histogram(aes(x = .resid), bins = 30)
```

#### Autocorrelation of Residuals
```{r m6_resid_acf}
feasts::ACF(residuals(m6, "innovation"), .resid) %>%  #innovation type is forecast using all prior information, regression is the mean
  autoplot()
```

The autocorrelation is pretty high in some places, this is not good.

##### Portmantaeau Test of Residual Autocorrelation
```{r m6_res_portmantaeau}
ljung_box(residuals(m6[[1, 1]])$.resid,
          lag = 16, #16 is a year
          dof = nrow(m6[[1, 1]]$fit$par) ) #dof of the fitted model
ljung_box(residuals(m6[[1, 1]])$.resid,
          lag = 16, #16 is a year
          dof = 6  ) #dof of the ARIMA component
ljung_box(residuals(m6[[1, 1]])$.resid,
          lag = 16, #16 is a year
          dof = nrow(residuals(m6[[1, 1]])) - nrow(m6[[1, 1]]$fit$par)  ) #dof of the ARIMA component
```


__WARNING: I need to look into the mathematics of Ljung-Box test as I'm not sure of the meaning of degrees of freedom when linear regression is involved.__

If the above tests are applied correctly then it means the autocorrelations are statistical significant; the residuals are not consistent with white noise.

#### Constant Variance Given Linear Predictor
Looking at this because that is the model assumption.

#### Additional Structure in Residuals
##### Between-Site Correlation
```{r m6_res_corrbetweenfarms}
resid_mat <- train %>% filtertrain() %>% tibble::rowid_to_column() %>%
  right_join(residuals(m6[[1, 1]]), by = "rowid") %>%
  as_tibble() %>% select(times, .resid, site) %>%
  pivot_wider(names_from = site, values_from = .resid) %>%
  select(c(-times))
cors <- cor(resid_mat, use = "complete.obs")

reorder_cormat <- function(cormat){
# Use correlation between variables as distance
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <- cormat[hc$order, hc$order]
}

cors %>%
  reorder_cormat() %>%
  reshape2::melt(value.name = "Pearson Correlation") %>%
  ggplot() +
  geom_tile(aes(x = Var1, y = Var2, fill = `Pearson Correlation`)) +
  geom_point(aes(x = Var1, y = Var2, shape = `Pearson Correlation` < 0.80)) +
  scale_fill_viridis_c() +
  scale_shape_manual(name = "Correlation < 0.8", values = c(NA, 19)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  ggtitle("Pearson Correlations of Residuals")

ordfarms <- colnames(cors %>% reorder_cormat())
ordfarm_cols <- viridis(length(ordfarms))
names(ordfarm_cols) <- ordfarms

pggpp %>%
  filter(sitenum == 1) %>%
  filter(yday %in% seq(1, 366, by = 8)) %>%
  ggplot() +
  facet_wrap(~year, scales = "free") +
  geom_line(aes(x = times, y = pg_24d.ydaymed, col = site),
            alpha = 0.3,
            lty = "dashed", size = 0.1) +
  geom_line(aes(x = times, y = pg_24d, col = site)) +
  scale_color_manual(values = ordfarm_cols) +
  guides(lty = guide_legend()) +
  ggtitle("Median pg_24d vs actual pg_24d")

```

Some correllation of residuals between sites. The geographic cluster of farms near Gundagai is prominent as having lots of correlation between residuals.

##### Association with yday
```{r m6_res_constantvar}
train %>% filtertrain() %>% tibble::rowid_to_column() %>%
  right_join(residuals(m6), by = "rowid") %>%
  ggplot() +
  facet_wrap(~year) +
  geom_hline(yintercept = 0) +
  geom_point(aes(x = yday, y = .resid))
```

There is definitely still structure in the residuals: clear correlation between residuals on each day. `yday` was not a predictor in the model though.

### Relation to model m1
```{r m6_vs_m1}
m1coefs <- as_tibble(summary(m1)$coefficients, rownames = "term") %>%
  rename(estimate = "Estimate",
         std.error = "Std. Error",
         statistic = "t value",
         p.value = "Pr(>|t|)")
m1coefs$term[[1]] <- "intercept"
left_join(tidy(m6[[1, 1]]),
          m1coefs,
          by = "term",
          suffix = c(".m6", ".m1")) %>%
  select(term, estimate.m1, estimate.m6)
```


The fitted coeficients are different between m1 and m6, however for the most part they are very similar in scale.

## Linear Regression with ARIMA errors and sites

```{r m7_parsearch}
filtertrain <- function(x){
  x <- x %>% filter(sitenum == 1,
                    #farm %in% c("BELL", "WILS", "GEDD"),
                    !is.na(pg_1to14m),
                    yday %in% seq(1, 366, by = 3 * 8), #reduce correlation in data fitting and reduce fitting time
                    !is.na(gpp)) # %>%
  #   mutate(recippg_1to5m.ydaymed = 1/pg_1to5m.ydaymed)
  # x$recippg_1to5m.ydaymed <- scale(x$recippg_1to5m.ydaymed)
  return(x)
}

glance(m6[[1, 1]])$AIC
bestscale = optimize(f = function(s) (extractAIC(m1, scale = s)[[2]] - glance(m6[[1, 1]])$AIC)^2,
      interval = c(0, 2))

foundmodel <- step(m1,
                   ~ . * site,
                   scale = bestscale$minimum,
                   direction = "both",
                   trace = 0)

formula(foundmodel)
```

Above uses `scale` argument to account for the correlation between observations when calculating AIC. I expect this to only work partially.

AIC optimisation suggests that interactions with site for *all* terms is useful. That is inconvenient when it comes to extracting a site effect. But anyway, let follow it and see if R2 or anything else improves too.

```{r m7_fit}
filtertrain <- function(x){
  x <- x %>% filter(sitenum == 1,
                    #farm %in% c("BELL", "WILS", "GEDD"),
                    !is.na(pg_1to14m),
                    yday %in% seq(1, 366, by = 3 * 8), #reduce correlation in data fitting and reduce fitting time
                    !is.na(gpp))
  return(x)
}
m7_sp <- ARIMA(box_cox(gpp / gpp.ydaymed, lambda = 0.2626) ~ 
              pg_1to5m * I(1 / pg_1to5m.ydaymed) * pg_24d * farm +  
              pdq())

m7 <- train %>% filtertrain() %>%
  tibble::rowid_to_column() %>% #reindex by approximate 8 day intervals (discrepancies at the end of every year)
  as_tsibble(key = NULL, index = "rowid") %>%
  ungroup() %>%
  model(m7_sp)

print(m7[[1, 1]])
tidy(m7[[1, 1]])
glance(m7[[1, 1]])

m7 %>%
  forecast(new_data = train %>% filtertrain() %>%
             #select(-gpp) %>% # I've checked that it doesn't use gpp
             tibble::rowid_to_column() %>%
             #filter(farm %in% c("GEDD")) %>% 
             update_tsibble(key = NULL, index = "rowid") %>%
             fill_gaps()
           ) %>%
  filter(!is.na(times)) %>%
  update_tsibble(key = "site", index = "times") %>%
  filter(farm %in% c("WILS", "GEDD", "MATH", "SMAR")) %>% 
  mutate(interval = hilo(.distribution, 95)) %>%
  ggplot() +
  facet_wrap(~farm, scales = "free_y") +
  geom_ribbon(aes(x = times, ymin = interval$.lower * gpp.ydaymed, ymax = interval$.upper * gpp.ydaymed), fill = "lightgrey") +
  scale_y_continuous(name = "GPP") +
  geom_line(aes(x = times, y = `gpp/gpp.ydaymed` * gpp.ydaymed), col = "black") + 
  geom_line(aes(x = times, y = gpp), col = "blue") +
  ggtitle("m7")
```

The above uses all possible interactions with `farm` as this was suggested by `step()` search with of a linear model.
The model takes a long time to fit (it fitted overnight).
From visual inspection the forecasts of `m6` and `m7` look identical.

### Site Coefficients
```{r displaysitecoeffs}
tidy(m7) %>%
  filter(grepl("farm", term)) %>%
  mutate(termshort = gsub("farm....", "", term)) %>%
  mutate(farm = substring(term, regexpr("farm", term) + 4, regexpr("farm....", term) +  7)) %>%
  ggplot() +
  facet_wrap(~termshort, scales = "free_y") +
  geom_hline(yintercept = 0, lty = "dashed") +
  geom_crossbar(aes(x = farm, y = estimate, ymin = estimate - 2 * std.error, ymax = estimate + 2 * std.error, fill = farm), fatten = 0.5) +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1))
```



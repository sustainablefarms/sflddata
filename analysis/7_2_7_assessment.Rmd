---
title: "Assessment of Two Species Models (experiment 7_2_7)"
author: "Kassel Hingee"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document: 
    collapsed: no
    number_sections: yes
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = rprojroot::find_root(rprojroot::has_file("DESCRIPTION")))
devtools::load_all(rprojroot::find_root(rprojroot::has_file("DESCRIPTION")))
knitr::opts_chunk$set(echo = FALSE)
library(tibble)
library(dplyr)
library(tidyr)
library(MCMCpack)
library(mclust)
library(corrplot)
library(coda)
library(runjags)
library(ggplot2)
library(patchwork)
```

```{r varname2type}
varname2type <- function(varnames){
  types <- case_when(
    grepl("lv.coef", varnames) ~ "LV Load",
    grepl("LV", varnames) ~ "LV",
    grepl("^(mu|tau)", varnames) ~ "Comm Param", #parameters of community distributions
    grepl("^u.b", varnames) ~ "Occu Coef",
    grepl("^v.b", varnames) ~ "Detn Coef",
    TRUE ~ "other"
    )
  return(types)
}
```

## Data Import

```{r importdata, echo = FALSE}
inputdata <- readRDS("./private/data/clean/7_2_4_input_data.rds")
# the following whittle out the covariates that we are interested in the residual diagnostics
detcovar <- model.matrix(~ SurveyYear + ModelSiteID + MeanWind +  MeanTime + MeanClouds + MeanTemp + ObserverId - 1,
             data = inputdata$insampledata$yXobs) %>%
  as_tibble() %>% rename(ModelSite = ModelSiteID)
occcovar <- model.matrix(~ StudyId + SurveyYear + ModelSiteID + os + ms * NMdetected + gc - 1,
             data = inputdata$insampledata$Xocc) %>%
  as_tibble() %>% rename(ModelSite = ModelSiteID)
```
```{r importmodelfits}
modelspecs <- readRDS("./tmpdata/7_2_7_modelspecs.rds")

filenames <- lapply(modelspecs, function(x) x$filename)

# test loading models
a <- vapply(filenames, file.exists, FUN.VALUE = FALSE)
stopifnot(all(a))

# load and remove crosscorrelation
fittedmods <- lapply(filenames, function(x) {
  fit <- readRDS(x)
  return(fit)})
```

```{r import_lpd}
lpds_l <- readRDS("./tmpdata/7_2_7_lpds.rds")
waics_l <- readRDS("./tmpdata/7_2_7_waics.rds")
```


```{r cleanersummaries}
cleanersummary <- function(fit) {
  vals <- fit$summaries %>%
    as_tibble(rownames = "bugsvarname") %>%
    mutate(AC_10 = case_when(
      is.finite(AC.400) ~ AC.400,
      TRUE ~ as.numeric(NA))) %>%
    mutate(varname = gsub("\\[.*\\]", "", bugsvarname)) %>%
    mutate(type = varname2type(bugsvarname))
  suppressWarnings(vals <- vals %>%
    mutate(SpeciesIdx = as.integer(gsub("(.*\\[|,.*\\])", "", bugsvarname))) %>%
    mutate(CovarIdx = as.integer(gsub("(.*\\[.*,|\\])", "", bugsvarname))) )
  vals <- vals %>%
    mutate(Species = fit$species[SpeciesIdx]) %>%
    mutate(Covariate = case_when(
      varname == "u.b" ~ colnames(fit$data$Xocc)[CovarIdx],
      varname == "v.b" ~ colnames(fit$data$Xobs)[CovarIdx],
      varname == "LV" ~ as.character(CovarIdx),
      TRUE ~ as.character(NA)
    )) %>% 
    dplyr::select(-SpeciesIdx, -CovarIdx)
  return(vals)
}
cleanersummary_l <- lapply(fittedmods, cleanersummary)
cleanersummaries <- bind_rows(cleanersummary_l, .id = "Model")
```


```{r mcmctime}
cat("MCMC time:\n")
lapply(fittedmods, function(x) {
  if (!is.null(x$timetaken)) {return(runjags::timestring(as.numeric(x$timetaken, units="secs")))}
  else return(NULL)})
```

The two species models were incredibly quick to fit.

## MCMC Assessment
### Autocorrelation 
```{r autocorr_fromsummary}
cleanersummaries %>%
  ggplot() +
  facet_grid(rows = vars(type), cols = vars(Model), scales = "free_y") +
  geom_histogram(aes(x = AC_10), bins = 30) +
  geom_vline(aes(xintercept = 0.1), col = "red") +
  scale_y_continuous(name = "Number of Parameters")
```

Autocorrelation across 10 samples was very low for all models.

### Convegence (Geweke)
See http://www.ugrad.stat.ubc.ca/R/library/coda/html/geweke.diag.html for a very quick description of this.
The Geweke values are Z-scores (technically t-distributed in this situation?).
95% of (independent) Geweke values should be within 2 standard deviations (i.e. just 2 for Z-scores) of the mean.
The below assumes the Geweke value for parameter is also *independent*, even in the situation of converged MCMC.

```{r gewekesumm}
gwk <- bind_rows(lapply(fittedmods, 
       function(x) enframe(geweke.diag(x, frac1=0.1, frac2=0.5)$z, name = "varname")),
       .id = "Model")
gwk %>%
  mutate(type = varname2type(varname)) %>%
  ggplot() +
  facet_grid(rows = vars(type), cols = vars(Model)) +
  geom_abline(slope = 1, intercept = 0, lty = "dashed") +
  geom_qq(aes(sample = value), shape = "+", size = 2) +
  coord_cartesian(ylim = c(-5, 5)) +
  ggtitle("QQ Plots of Geweke Statistics for Each Model Parameter")
```


```{r swstatistics, rows.print = 14}
gwk %>%
  mutate(type = varname2type(varname)) %>%
  group_by(Model, type) %>%
  filter(Model != "interceptsonly") %>%
  filter(Model != "ms", type != "Detn Coef") %>%
  summarise(swp = shapiro.test(value)$p.value) %>%
  ggplot() +
  facet_wrap(~type) +
  geom_vline(aes(xintercept = 0.01)) +
  geom_point(aes(y = Model, x = swp, col = Model)) + 
  scale_x_continuous(name = "Shapiro-Wilk p-value",
                     trans = "identity") +
  ggtitle("Geweke Convergence Statistics Normality Tests",
          subtitle = "0.01 threshold shown")
```

Convergence is ok for all models. However, the qq plot of os_msnm_gc has me worried.

### Multi Chain Gelman-Rubin Statistic (aka "Rhat" or psrf)
Values less than 1.1 are desired.

```{r gelmanrubin}
psrfs <- lapply(fittedmods, function(x) as_tibble(x$summaries[, "psrf"], rownames = "varname"))
psrfs_df <- bind_rows(psrfs, .id = "Model")

psrfs_df %>%
  mutate(type = varname2type(varname)) %>%
  ggplot() +
  facet_grid(rows = vars(type), cols = vars(Model), scales = "free_y") +
  geom_point(aes(y = varname, x = value) ) +
  geom_vline(xintercept = 1.1, col = "blue", lty = "dashed") +
  scale_y_discrete(name = "Variable Name") +
  scale_x_continuous(name = "Gelman-Rubin Statistic")
```

The MCMC chains converged very well according to all parameters and the Gelman-Rubin statistic.

### Conclusions
All MCMC chains are usuable.

## Compare Covariate Loadings
```{r occupancycovariateloadings}
cleanersummaries %>%
  dplyr::filter(varname == "u.b") %>%
  ggplot() +
  facet_wrap(vars(Covariate), nrow = 1) +
  geom_hline(yintercept = 0) +
  geom_pointrange(aes(x = Species,
                 ymin = Lower95,
                 ymax = Upper95,
                 y = Median,
                 col = Model,
                 group = Model),
             shape = "|",
             size = 1,
             position = position_dodge(width = 1)) +
  scale_color_viridis_d() +
  scale_y_continuous(name = "95% credible interval with median") +
  coord_flip() +
  ggtitle("Occupancy Covariate Loadings per Species and Model")
```
  
Occupancy covariate loadings are very similar between models that incorporate the relevant covariate. Models os_ms_nm_gc, os_ms_nm, os_ms, ms appear to have a slighlty different loading for midstorey than the models msnm_time, msnm_time_temp, msnm_timetemp, os_msnm_gc, which all have an interaction term between midstorey and noisy minor.
A similar situation for NMdetected for models with or without interaction with midstorey.

The 95% credible intervals for ms and NMdetected do not cross zero for either species. The interaction ms:NMdetected is non-zero for Willie Wagtail, and the gc loading is non-zero for Galah. 
The credible interval of os loading crosses zero for most models and species.
Models that include the covariates gc, ms, ms:NMdetected or NMdetected are significantly different to the intercepts-only model. *I would expect these models to have different lpd estimates.*

```{r detectioncovariateloadings}
cleanersummaries %>%
  dplyr::filter(varname == "v.b") %>%
  ggplot() +
  facet_wrap(vars(Covariate), nrow = 1) +
  geom_hline(yintercept = 0) +
  geom_pointrange(aes(x = Species,
                 ymin = Lower95,
                 ymax = Upper95,
                 y = Median,
                 col = Model,
                 group = Model),
             shape = "|",
             size = 1,
             position = position_dodge(width = 1)) +
  scale_y_continuous(name = "95% credible interval with median") +
  coord_flip() +
  scale_color_viridis_d() +
  ggtitle("Detection Covariate Loadings per Species and Model")
```

The 95% credible interval of MeanTime does not include 0, thus the relevant models are different to the intercepts-only model.
The inclusion of interaction between MeanTime and MeanTemp creates much broader credible intervals for MeanTemp loading. The intervals intersect the loadings for the models with MeanTemp but without the interaction.
The interaction loadings all cross zero, suggesting that the interaction term is not needed (if the model assumptions are correct).

## Model Comparisons
### Log Posterior Density
```{r holdoutlpd}
lpds_df <- as_tibble(do.call(cbind, lapply(lpds_l, function(x) x$lpds)))
melpd <- data.frame(Estimate = apply(lpds_df, 2, mean),
                    SE = apply(lpds_df, 2, sd) / sqrt(nrow(lpds_df)))
# 
# mean_2se <- function(x, mult = 2){
#   n <- length(x)
#   xbar <- sum(x)/n
#   sd <- sqrt(sum((x - xbar)^2)/(n - 1))
#   return(c(ymin = xbar - mult * sd / sqrt(n),
#            y = xbar,
#            ymax = xbar + mult * sd / sqrt(n)))
# }

lpds_df %>%
  as_tibble() %>%
  rowid_to_column(var = "HoldOutModelSite") %>%
  tidyr::pivot_longer(-HoldOutModelSite, names_to = "model", values_to = "Site_lpd") %>%
  ggplot() +
  facet_grid(rows = vars(model), scales = "free_y") +
  geom_violin(aes(x = Site_lpd, y = model), adjust = 1/2) +
  stat_summary(aes(x = Site_lpd, y = model),
               fun.data=mean_se, fun.args = list(mult=2), geom="crossbar", width=0.2 ) +
  stat_summary(aes(x = Site_lpd, y = model),
               fun = median, geom = "point", shape = 23, size = 2) +
  ggtitle("Log-Posterior Density of Holdout ModelSites")
```

Mean lpd for holdout data is not very discriminatory, yet the distributions of lpd are obviously very different. 
Median lpd is a little more discriminatory.

Below is the lpd computed for each ModelSite for both the InSample and out of sample data.

There is very strange, multimodal behavior. Suggests ModelSites are clustering in their behavior.

```{r distributioninsampleoutsamplelpd}
insamplelpds_df <- do.call(cbind, lapply(waics_l, function(x) x$waic$pointwise[, "elpd_waic"])) %>%
  as_tibble() %>% mutate(InSample = TRUE)

df <- lpds_df %>% as_tibble() %>% mutate(InSample = FALSE)

df <- bind_rows(insamplelpds_df, df)


df %>%
  as_tibble() %>%
  tidyr::pivot_longer(-InSample, names_to = "model", values_to = "site_lpd") %>%
  ggplot() +
  facet_grid(rows = vars(model), scales = "free_y", switch = "y") +
  geom_violin(aes(x = site_lpd, y = InSample, col = InSample),
              adjust = 1/2,
              position = position_dodge(1)) +
  stat_summary(aes(x = site_lpd, y = InSample, group = InSample),
               fun.data=mean_se, fun.args = list(mult=2), geom="crossbar", width=0.2 ) +
  stat_summary(aes(x = site_lpd, y = InSample, group = InSample),
               fun = median, geom = "point", shape = 23, size = 2) +
  theme(strip.text.y.left = element_text(angle = 0)) +
  ggtitle("Log Posterior Density of ModelSites")
```

There are clear losers in some of these models (msnm_timetemp), but on the whole they are all very similar. Note that WAIC values are different because they adjust for the insample bias.

There is something funky with the Median values. The 0.5 quantile from geom_violin is different to the median! I think this would be due to the smoothed density!

```{r waics}
waics_average_elpd_per_point <- lapply(waics_l, function(x) x$waic$estimates["elpd_waic", ]/nrow(x$waic$pointwise))
waics_average_elpd_per_point <- simplify2array(waics_average_elpd_per_point)
waics_average_elpd_per_point <- t(waics_average_elpd_per_point) %>% as_tibble(rownames = "Model")
waics_average_elpd_per_point$type = "WAIC"

loo_average_elpd_per_point <- lapply(waics_l, function(x) x$loo$estimates["elpd_loo", ]/nrow(x$waic$pointwise))
loo_average_elpd_per_point <- simplify2array(loo_average_elpd_per_point)
loo_average_elpd_per_point <- t(loo_average_elpd_per_point) %>% as_tibble(rownames = "Model")
loo_average_elpd_per_point$type = "LOO-PSIS"

melpd$type <- "Holdout"
melpd <- as_tibble(melpd, rownames = "Model")

df <- bind_rows(waics_average_elpd_per_point, loo_average_elpd_per_point, melpd)

df %>%
  ggplot() +
  geom_errorbar(aes(x = Model, ymax = Estimate +  2*SE, ymin = Estimate - 2*SE, col = type, lty = type)) +
  geom_point(aes(x = Model, y = Estimate, col = type), position = position_jitter(width = 0.1)) +
  coord_flip() +
  ggtitle("Estimates of Log Posterior Density for a new ModelSite",
          subtitle = "Holdout estimate is the mean lpd of holdout sites")
```

Many of the models are only as good as the intercept-only model according to the WAIC.
Curiously the mean holdout lpd increases slightly for the models with the worst WAIC.
This is *not* due to the WAIC bias adjustment.
**How is this possible?**

### Expected Number of Detections for Each Species
```{r numdetections}
obsdetections <- colSums(fittedmods[[1]]$data$y)

Edetections <- lapply(fittedmods,
                      function(x) colSums(Endetect_modelsite(x,
                                            conditionalLV = (!is.null(x$data$nlv) && (x$data$nlv > 0) ))
                                          )
                      )
Edetections <- simplify2array(Edetections)
# Edetections[names(obsdetections), "Observed"] <- obsdetections



Edetections %>% 
  as_tibble(Edetections, rownames = "Species") %>%
  tidyr::pivot_longer(cols = -Species, names_to = "Model", values_to = "E_Ndetections") %>%
  ggplot() +
  geom_point(aes(x = E_Ndetections, y = Species, col = Model, shape = Model)) +
  scale_color_viridis_d() +
  scale_shape_manual(values = rep(0:5, 3)) +
  geom_point(aes(x = Detections, y = Species), data = obsdetections %>% enframe(name = "Species", value = "Detections"),
             shape = "|",
             size = 10,
             inherit.aes = FALSE) +
  ggtitle("Expected Number of Detections vs Observed")
```


### Quick check of residual distribution for all models
Note that there are too many data points to apply the shapiro-Wilks normality test directly, however the Anderson-Darling test can be applied. I do not know enough about this test yet to know if it can be fully trusted.

```{r all_resid_occ_normality}
resid_occ_l <- lapply(fittedmods, ds_occupancy_residuals.fit, type = "median", seed = 321, conditionalLV = FALSE)
# vapply(resid_occ_l, function(x) shapiro.test(sample(unlist(x[, -1]), 5000))$p.value, FUN.VALUE = 3.3)
vapply(resid_occ_l, function(x) goftest::ad.test(unlist(x[, -1]))$p.value, FUN.VALUE = 3.3)
as_tibble(lapply(resid_occ_l, function(x) unlist(x[, -1]))) %>%
  tidyr::pivot_longer(everything(), names_to = "Model", values_to = "Residual") %>%
  ggplot() +
  geom_qq(aes(sample = Residual)) +
  geom_abline(slope = 1, intercept = 0, lty = "dashed") +
  facet_grid(cols = vars(Model)) +
  ggtitle("Occupancy Residual Disribution: qq plots")
```


```{r all_resid_det_normality}
resid_det_l <- lapply(fittedmods, ds_detection_residuals.fit, type = "median", seed = 321)
vapply(resid_det_l, function(x) {
  vals <- unlist(x[, -1])
  vals <- vals[!is.na(vals)]
  goftest::ad.test(vals)$p.value},
  FUN.VALUE = 3.3)
as_tibble(lapply(resid_det_l, function(x) unlist(x[, -1]))) %>%
  pivot_longer(everything(), names_to = "Model", values_to = "Residual") %>%
  ggplot() +
  geom_qq(aes(sample = Residual)) +
  geom_abline(slope = 1, intercept = 0, lty = "dashed") +
  facet_grid(cols = vars(Model)) +
  ggtitle("Detection Residual Distribution: qq plots")
```



### Occupancy Residuals
```{r residocc_plots_manymodels}
seeds = c(321, 120, 6545, 65498, 63)
df_l <- lapply(seeds, function(x) {
  resid_occ_l <- lapply(fittedmods, ds_occupancy_residuals.fit, type = "median", seed = x, conditionalLV = FALSE)
  resid_occ_df <- bind_rows(resid_occ_l, .id = "Model")
  df <- resid_occ_df %>%
    pivot_longer(-c(ModelSite, Model),
                   names_to = "Species",
                   values_to = "Residual",
                   values_drop_na = TRUE) %>%
    left_join(occcovar %>% 
                pivot_longer(-ModelSite,
                   names_to = "Covariate",
                   values_to = "CovariateValue"),
              by = "ModelSite")
  return(df)
})
names(df_l) <- seeds
df <- bind_rows(df_l, .id = "seed")

treatfactor <- df %>%
  group_by(Covariate) %>%
  summarise(nvals = n_distinct(CovariateValue), .groups = "drop_last") %>%
  filter(nvals <= 10) %>%
  dplyr::select(Covariate) %>%
  unlist()

plt <- df %>%
  ggplot() +
  ggplot2::geom_point(aes(x = CovariateValue, y = Residual), data = function(x) x[x$seed == seeds[[1]], ])

# gam smooths for continuous variables
plt <- plt + ggplot2::geom_smooth(aes(x = CovariateValue, y = Residual, col = seed),
                                  data = function(x) x %>% dplyr::filter(!(Covariate %in% treatfactor)),
                                  method = "gam", level = 0.95, formula = y ~ s(x, bs = "cs"))

# mean + 2SE summaries for discrete variables
plt <- plt + ggplot2::stat_summary(aes(x = CovariateValue, y = Residual, col = seed),
                                  data = function(x) x %>% dplyr::filter(Covariate %in% treatfactor),
                                  geom = "pointrange",
                                  fun.data = mean_se,
                                  fun.args = list(mult = 2),
                                  position = position_dodge(width = 0.1, preserve = "total"),
                                  alpha = 0.8,
                                  lwd = 2,
                                  fatten = 1,
                                  show.legend = FALSE)

plt <- plt +
  ggplot2::facet_grid(rows = vars(Model), cols = vars(Covariate), as.table = TRUE, scales = "free_x") +
  ggplot2::geom_hline(yintercept = 0, col = "blue", lty = "dashed") +
  ggplot2::scale_x_continuous(name = "Covariate Value") +
  scale_y_continuous(name = "Occupancy Residual")
plt + coord_cartesian(ylim = c(-0.1, 0.1)) + ggtitle("Occupancy Residuals vs Covariates") +
  theme(strip.text.y.right = element_text(angle = 0))
```

Suggests that SurveyYear is very important. And the model is not very good: there are systematic differences between Studies.

### Detection Residuals
```{r residdet_plots_manymodels}
seeds = c(321, 120, 6545, 65498, 63)
df_l <- lapply(seeds, function(x) {
  resid_det_l <- lapply(fittedmods, ds_detection_residuals.fit, type = "median", seed = x)
  resid_det_df <- bind_rows(resid_det_l, .id = "Model")
  df <- resid_det_df %>%
    pivot_longer(-c(ModelSite, Model),
                   names_to = "Species",
                   values_to = "Residual",
                   values_drop_na = TRUE) %>%
    left_join(detcovar %>% 
                pivot_longer(-ModelSite,
                   names_to = "Covariate",
                   values_to = "CovariateValue"),
              by = "ModelSite")
  return(df)
})
names(df_l) <- seeds
df_det <- bind_rows(df_l, .id = "seed")

treatfactor <- df_det %>%
  group_by(Covariate) %>%
  summarise(nvals = n_distinct(CovariateValue), .groups = "drop_last") %>%
  filter(nvals <= 10) %>%
  dplyr::select(Covariate) %>%
  unlist()

plt <- df_det %>%
  ggplot() +
  ggplot2::geom_point(aes(x = CovariateValue, y = Residual), data = function(x) x[x$seed == seeds[[1]], ])

# gam smooths for continuous variables
plt <- plt + ggplot2::geom_smooth(aes(x = CovariateValue, y = Residual, col = seed),
                                  data = function(x) x %>% dplyr::filter(!(Covariate %in% treatfactor)),
                                  method = "gam", level = 0.95, formula = y ~ s(x, bs = "cs"))

# mean + 2SE summaries for discrete variables
plt <- plt + ggplot2::stat_summary(aes(x = CovariateValue, y = Residual, col = seed),
                                  data = function(x) x %>% dplyr::filter(Covariate %in% treatfactor),
                                  geom = "pointrange",
                                  fun.data = mean_se,
                                  fun.args = list(mult = 2),
                                  position = position_dodge(width = 0.1, preserve = "total"),
                                  alpha = 0.8,
                                  lwd = 2,
                                  fatten = 1,
                                  show.legend = FALSE)

plt <- plt +
  ggplot2::facet_grid(rows = vars(Model), cols = vars(Covariate), as.table = TRUE, scales = "free_x") +
  ggplot2::geom_hline(yintercept = 0, col = "blue", lty = "dashed") +
  ggplot2::scale_x_continuous(name = "Covariate Value") +
  scale_y_continuous(name = "Detection Residual")
plt + coord_cartesian(ylim = c(-0.1, 0.1)) + ggtitle("Detection Residuals vs Covariates")  +
  theme(strip.text.y.right = element_text(angle = 0))
```


## Conclusions
Log posterior density and WAIC behaved very strangely for these models.


### Next Steps

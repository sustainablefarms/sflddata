---
title: "Fit GPP using Linear Models and Lags"
author: "Kassel Hingee"
date: "15/04/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
out <- lapply(c("sf", "tsibble", 'lubridate', "viridis",
                'ggplot2', 'tidyr', 'grid', 'gridExtra', 
                'feasts', 'dplyr', 'gtable', 'fable',
                'mgcv', "raster", "sf", "ncdf4"),
       library, character.only = TRUE)
```
```{r setup2, include = FALSE}
out <- lapply(paste0("./R/", list.files("./R/")), source)
```

## Plan
Now that locations are not farms. Need

1. defensible covariate selection
   1. some checking of model suitability
   2. include soil moisture, sines and cosines
2. selection of summary of residuals for plugging into bigger model (by investigating covariance with other predictors?)
3. Export these summaries for every pixel.

## Preparation
### Load Nice Data Created in 1_8_GPP_PG_sampledlocs.Rmd
```{r readdatain}
longdata <- readRDS("./tmpdata/gpp_pg_woody_ssoil_sampledloc.rds")
```

### Add Derived Values
```{r derivedvals}
# add times breakdowns
longdata <- longdata %>%
  mutate(yday = yday(date),
         year = year(date)) %>%
# Add cumulative rainfalls
  group_by(pointid) %>%
  mutate(pg_cumsum = cumsum(pg)) %>%
  mutate(pg_8d = (pg_cumsum - lag(pg_cumsum, n = 8))/8.0,
         #0 to day 7 (8 days) cumulative rainfall to correspond with gaps in GPP
         pg_24d = (pg_cumsum - lag(pg_cumsum, n = 3*8)) / 24) %>% 
         # every 24 days (3*8) of GPP should be less correlated (given average GPP), this pg_24d corresponds to the rain through that period
  mutate(pg_32d = (pg_cumsum - lag(pg_cumsum, n = 1 * 4 * 8)) / (1 * 4 * 8),
         pg_64d = (pg_cumsum - lag(pg_cumsum, n = 2 * 4 * 8)) / (2 * 4 * 8),
         pg_96d = (pg_cumsum - lag(pg_cumsum, n = 3 * 4 * 8)) / (3 * 4 * 8),
         pg_128d = (pg_cumsum - lag(pg_cumsum, n = 4 * 4 * 8)) / (4 * 4 * 8),
         pg_160d = (pg_cumsum - lag(pg_cumsum, n = 5 * 4 * 8)) / (5 * 4 * 8),
         pg_192d = (pg_cumsum - lag(pg_cumsum, n = 6 * 4 * 8)) / (6 * 4 * 8),
         pg_224d = (pg_cumsum - lag(pg_cumsum, n = 7 * 4 * 8)) / (7 * 4 * 8),
         pg_256d = (pg_cumsum - lag(pg_cumsum, n = 8 * 4 * 8)) / (8 * 4 * 8),
         pg_320d = (pg_cumsum - lag(pg_cumsum, n = 10 * 4 * 8)) / (10 * 4 * 8),
         pg_384d = (pg_cumsum - lag(pg_cumsum, n = 12 * 4 * 8)) / (12 * 4 * 8),
         pg_448d = (pg_cumsum - lag(pg_cumsum, n = 15 * 4 * 8)) / (15 * 4 * 8),
           ) %>%
  ungroup()

# filter down to 8 day intevals 
longdata <- longdata %>%
  filter(yday %in% seq(1, 366, by = 8))

# gpp lags
longdata <- longdata %>%
  group_by(pointid) %>%
  mutate(gpp.1 = lag(gpp, n = 1),
         gpp.2 = lag(gpp, n = 2),
         gpp.3 = lag(gpp, n = 3),
         gpp.4 = lag(gpp, n = 4),
         gpp.5 = lag(gpp, n = 5),
         gpp.6 = lag(gpp, n = 6),
         gpp.7 = lag(gpp, n = 7),
         gpp.46 = lag(gpp, n = 46)) %>%
  ungroup()

longdata <- tsibble(longdata, index = date, key = pointid)
```

#### Add Sinusoidal Covariates
```{r addsinesandcosines}
id20_gpp <- (longdata %>% filter(pointid == 20, year <= 2019, year >= 2001))$gpp
id20f <- fft(id20_gpp)
# first entry of id20f is the coefficient of exp(0) in decomposition of id20's gpp
# second entry of id20f is the coefficient of exp(2 * pi * h) in breakdown of id20's gpp, corresponds to period of length of id20_gpp
# 3rd entry of id20f is the coefficient of exp(2 * pi * 2 * h), which corresponds to period of half the length of id20_gpp

# in the following 874==length of id20_gpp
# 19 == number of years in the data. For a yearly thing we want something with a period of 19th of the length of id20_gpp (or a multiple). This is index 20 of id20f
plot(1:874, id20_gpp)
lines(1:874, Re(fft(id20f, inverse = TRUE)) / 874)
# the lower frequency year-compatible component
lines(1:874, Re(fft(replace(id20f, c(2:19,21:874), 0) , inverse = TRUE)) / 874, col = "green")
#the highest frequency component that is periodic in a year
lines(1:874, Re(fft(replace(id20f, c(2:855,857:874), 0) , inverse = TRUE)) / 874, col = "blue")
# lines(1:874, Re(fft(replace(id20f, 20, 0) , inverse = TRUE)) / 874, col = "blue")

# We can re-write the first annual component as a function yday, and scale by 1/874:
par(mfrow = c(1, 2))
plot(1:(46 * 19), (1 / 874) * Re(id20f[20]*exp(2 * pi * 1i * 8 * rep(1:46 - 1, 19) / 365.25)),
     main = "First Yearly Component")
lines(1:874, Re(fft(replace(id20f, c(1:19,21:874), 0) , inverse = TRUE)) / 874, col = "green")
plot(c1vals <-  (1 / 874) * Re(id20f[20]*exp(2 * pi * 1i * 8 * rep(1:46 - 1, 19) / 365.25)),
Re(fft(replace(id20f, c(1:19,21:874), 0) , inverse = TRUE)) / 874,
xlab = "Interpreted Values", ylab = "Actual Values", main = "First Yearly Component")

# The following plot suggests that only the first 5 components matter (note the values are mirrored)
plot(1:874, abs(id20f))
points(1 + 19*(1:45), abs(id20f[1 + 19*(1:45)]), col = "red", pch = "+")


# That suggests we add 3 sinusoidal components: 
longdata <- longdata %>%
  mutate(yrly.1 = Re(id20f[1 + 19] * exp(2 * pi * 1i * yday / 365.25)),
         yrly.2 = Re(id20f[1 + 19 * 2] * exp(2 * pi * 1i * yday / 365.25)),
         yrly.3 = Re(id20f[1 + 19 * 3] * exp(2 * pi * 1i * yday / 365.25)),
         sine.1 = sin(2* pi * yday / 365.25),
         sine.2 = sin(2* pi * 2 * yday / 365.25),
         sine.3 = sin(2* pi * 3 * yday / 365.25),
         cosine.1 = cos(2* pi * yday / 365.25),
         cosine.2 = cos(2* pi * 2 * yday / 365.25),
         cosine.3 = cos(2* pi * 3 * yday / 365.25)
         )
# longdata %>%
#   ggplot() +
#   geom_point(aes(x = date, y = yrly.1))
```


### Train / Test Split
```{r preparetraintest}
train <- longdata %>%
  filter(yday %in% seq(1, 366, by = 8)) %>%
  filter_index(. ~ "2016-12-31") %>%
  dplyr::select(-pg_cumsum) 
# simple median of train set
ydaymedians.train <- train %>%
  as_tibble() %>% #stop the date column from being preserved
  filter(yday %in% seq(1, 366, by = 8)) %>%
  group_by(pointid, yday) %>%
  summarise(gpp.ydaymed = median(gpp),
            pg_8d.ydaymed = median(pg_8d),
            pg_24d.ydaymed = median(pg_24d),
            pg_160d.ydaymed = median(pg_160d, na.rm = TRUE)) %>%
  ungroup()
train <- left_join(train, ydaymedians.train, by = c("pointid", "yday"))

test <- longdata %>%
  filter_index("2017-01-01" ~ .) %>%
  filter(yday %in% seq(1, 366, by = 8)) %>%
  dplyr::select(-pg_cumsum) %>%
  left_join(ydaymedians.train, by = c("pointid", "yday"))
```

## Model Fitting
```{r covariate_collections}
pgnames <- grep("pg_", names(train), value = TRUE)
gppnames <- grep("gpp.", names(train), value = TRUE, fixed = TRUE)
sinunames <- c(grep("yrly.", names(train), value = TRUE, fixed = TRUE),
               grep("sine.", names(train), value = TRUE, fixed = TRUE))
othernames <- c("yday", "woody", "year", "pointid", "ssoil")
allcovariatenames <- c(pgnames, gppnames, othernames)
mediannames <- grep(".ydaymed$",allcovariatenames, value = TRUE)
```

### Transform GPP to Look Normal, Exponential, Gamma or Inverse Gaussian
Caveat: it is interesting to see that the best distibution appears to be a Gamma, but in regression it is actually the distribution of the residuals that is important.

```{r gpptransform_to_normal}
par(mfrow = c(2, 2))
hist(train$gpp)
hist(log(train$gpp))
hist(forecast::BoxCox(train$gpp, lambda = "auto"))
qqnorm(log(train$gpp), ylab = "log(gpp) sample quantiles"); abline(a = 0, b = 1)

hist(forecast::BoxCox(train$gpp + 1, lambda = "auto"))

hist(train$gpp - lag(train$gpp))
qqnorm(train$gpp - lag(train$gpp), ylab = "gpp_diff sample quantiles");  abline(a = 0, b = 1)
hist(forecast::BoxCox(train$gpp - lag(train$gpp), lambda = "auto"))
qqnorm(scale(forecast::BoxCox(train$gpp - lag(train$gpp), lambda = "auto")),
       ylab = "boxcox(gpp_diff) sample quantiles");  abline(a = 0, b = 1)


hist(train$gpp / train$gpp.ydaymed)
hist(log(train$gpp / train$gpp.ydaymed))
qqnorm(log(train$gpp / train$gpp.ydaymed));  abline(a = 0, b = 1)
qqnorm(log(train$gpp / train$gpp.ydaymed)[log(train$gpp / train$gpp.ydaymed) >= -3]);  abline(a = 0, b = 1)
```

In the above the difference, gpp - lag(gpp), is not a good fit for Gaussian distribution. It appears to have too large tails, and too small shoulders.

```{r isgppexponential}
fit_exp <- MASS::fitdistr(train$gpp, "exponential")

hist(train$gpp, freq = FALSE, breaks = 100, xlim = c(0, quantile(train$gpp, 0.99)))
curve(dexp(x, rate = fit_exp$estimate), from = 0, col = "red", add = TRUE)
qqplot(train$gpp, rexp(1000, rate = fit_exp$estimate), main = "VS Exponential Distribution"); abline(a = 0, b = 1)

ks.test(train$gpp, "pexp", fit_exp$estimate) #rejects exponential distribution hypothesis
```

```{r isgppgamma}
fit_gamma <- MASS::fitdistr(train$gpp, "gamma")

ks.test(train$gpp, "pgamma", shape = fit_gamma$estimate[["shape"]], rate = fit_gamma$estimate[["rate"]]) #rejects gamma distribution too

par(mfrow = c(1, 3))
hist(train$gpp, freq = FALSE, breaks = 100, xlim = c(0, quantile(train$gpp, 0.99)))
curve(dgamma(x, shape = fit_gamma$estimate[["shape"]], rate = fit_gamma$estimate[["rate"]]), from = 0, col = "red", add = TRUE)
qqplot(train$gpp, rgamma(1000, shape = fit_gamma$estimate[["shape"]],
                         rate = fit_gamma$estimate[["rate"]]),
       main = "VS Gamma Distribution"); abline(a = 0, b = 1)
qqplot(train$gpp, rgamma(1000, shape = fit_gamma$estimate[["shape"]],
                         rate = fit_gamma$estimate[["rate"]]),
       main = "VS Gamma Distribution",
       xlim = c(0, 5), ylim = c(0, 5)); abline(a = 0, b = 1)

```

Gamma and exponential distributions appear to be much closer than the log-normal distributions. The fitted gamma distribution, in particular, looks good so it makes sense to use a Gamma family in the GLM. The link function can be inverse, identity or log; I am not sure what I would choose.

__How do the two parameters of the Gamma distribution get fit? The mean depends on the predictors and how about a second parameter?__

```{r isgppinversegaussian}
fit_nig <- GeneralizedHyperbolic::nigFit(train$gpp, plots = TRUE)
```

I'm not sure what to make of the above fit to an inverse Gaussian. Fit looks pretty bad in the q-q plot.

### m14: GLM with Gamma Family and Log Link
Model fitting with identity link was really difficult - lots of errors finding coefficients. I think it is more stable using a log link because this makes sure that the mean is always positive.
However it still has major convergence issues.

#### Test Model Selection Using a Subset of Predictors
```{r lognormal_modelselection_test}
trainfilt <- train %>% filter_all(all_vars(!is.na(.)))
fmla <- as.formula("log(gpp) ~ 1 + (pg_8d + pg_24d + pg_32d + log(gpp.1) + log(gpp.2) + log(gpp.3) + log(gpp.4) + log(gpp.ydaymed) ) * yday * woody * pointid")
fmla <- as.formula(paste0("gpp ~ 1 + (",
                          paste(pgnames, collapse = "+"),
                          ") + (",
                          paste0(gppnames, collapse = " + "),
                          ") + (",
                          paste0(othernames, collapse = " + "),
                          ") + (",
                          paste0(sinunames, collapse = " + "),
                          ")"))
mod <- glm("gpp ~ 1",
    family = Gamma(link = "log"),
    data = trainfilt)
best_gamma <- MASS::stepAIC(mod,
                      scope = fmla,
                      trace = 1,
                      direction = "both",
                      k = log(nrow(trainfilt))) #this makes it BIC
saveRDS(best_gamma, "./tmpdata/best_gamma.rds")
summary(best_gamma)
plot(best_gamma, ask = FALSE)
qqnorm(best_gamma$residuals, ylim = c(-5, 5))
hist(best_gamma$residuals, xlim = c(-5, 5), breaks = 30)

trainfilt$fitted <- fitted(best_gamma)
trainfilt %>%
  dplyr::filter(pointid == 1, year >= 2015) %>%
  pivot_longer(cols = c(gpp, fitted)) %>%
  ggplot() +
  facet_wrap(~pointid) +
  geom_line(aes(x = date, y = value, col = name))

trainfilt %>%
  mutate(resid = gpp - fitted) %>%
  dplyr::filter(pointid == 1, year >= 2015) %>%
  ggplot() +
  facet_wrap(~pointid) +
  geom_hline(aes(yintercept = 0), col = "grey", lty = "dashed") +
  geom_line(aes(x = date, y = resid)) +
  geom_point(aes(x = date, y = resid))
```

```{r m14_gamma}
fmla <- as.formula(paste0("gpp ~ 1 + (",
                          paste(pgnames, collapse = "+"),
                          ") * (",
                          paste0(gppnames, collapse = " + "),
                          ") * (",
                          paste0(othernames, collapse = " + "),
                          ")")
                   )
# fmla <- as.formula("gpp ~ 1 + sin(yday * 2 * pi / 365) * yday * pg_24d + log(gpp.1)")
trainfilt <- train %>% filter_all(all_vars(!is.na(.)))
m14_proto <- glm(fmla,
                 family = gaussian(link = "log"),
                 data = trainfilt)
m14 <- glm(fmla,
    family = Gamma(link = "log"),
    start = m14_proto$coefficients,
    mustart = rep(2.4, nrow(trainfilt)),
    etastart = rep(2.4, nrow(trainfilt)),
    data = trainfilt)
summary(m14)
```

```{r m14_viewfit}
plot(m14)

trainfilt <- train %>% filter_all(all_vars(!is.na(.)))
trainfilt$fitted <- fitted(m14)
trainfilt %>%
  dplyr::filter(pointid == 1) %>%
  pivot_longer(cols = c(gpp, fitted)) %>%
  ggplot() +
  facet_wrap(~pointid) +
  geom_line(aes(x = date, y = value, col = name))

trainfilt %>%
  mutate(resid = gpp - fitted) %>%
  dplyr::filter(pointid == 1, year >= 2015) %>%
  ggplot() +
  facet_wrap(~pointid) +
  geom_hline(aes(yintercept = 0), col = "grey", lty = "dashed") +
  geom_line(aes(x = date, y = resid)) +
  geom_point(aes(x = date, y = resid))
```

### m15: Log Normal Model Interaction Between Rain and GPP

#### Test Model Selection Using a Subset of Predictors
```{r lognormal_modelselection_test}
trainfilt <- train %>% filter_all(all_vars(!is.na(.)))
fmla <- as.formula("log(gpp) ~ 1 + (pg_8d + pg_24d + pg_32d + log(gpp.1) + log(gpp.2) + log(gpp.3) + log(gpp.4) + log(gpp.ydaymed) ) * yday * woody * pointid")
fmla <- as.formula(paste0("gpp ~ 1 + (",
                          paste(pgnames, collapse = "+"),
                          ") + (",
                          paste0(gppnames, collapse = " + "),
                          ") + (",
                          paste0(othernames, collapse = " + "),
                          ")"))
mod <- lm("log(gpp) ~ 1",
    data = trainfilt)
best <- MASS::stepAIC(mod,
                      scope = fmla,
                      trace = 1,
                      direction = "both",
                      k = log(nrow(trainfilt))) #this makes it BIC
plot(best, ask = FALSE)
qqnorm(best$residuals, ylim = c(-5, 5))
hist(best$residuals, xlim = c(-5, 5), breaks = 30)
```

```{r m15_lognormal}
fmla <- as.formula(paste0("log(gpp) ~ 1 + (",
                          paste(pgnames, collapse = "+"),
                          ") * (",
                          paste0("log(",gppnames,")", collapse = " + "),
                          ") * (",
                          paste0(othernames, collapse = " + "),
                          ")")
                   )
m15 <- lm(fmla,
    data = train %>% filter_all(all_vars(!is.na(.))))
summary(m15)
```

```{r m15_viewfit}
plot(m15)

trainfilt <- train %>% filter_all(all_vars(!is.na(.)))
trainfilt$fitted <- fitted(m15)
trainfilt %>%
  dplyr::filter(pointid == 1) %>%
  pivot_longer(cols = c(gpp, fitted)) %>%
  ggplot() +
  facet_wrap(~pointid) +
  geom_line(aes(x = date, y = value, col = name))

trainfilt %>%
  mutate(resid = gpp - fitted) %>%
  dplyr::filter(pointid == 1, year >= 2015) %>%
  ggplot() +
  facet_wrap(~pointid) +
  geom_hline(aes(yintercept = 0), col = "grey", lty = "dashed") +
  geom_line(aes(x = date, y = resid)) +
  geom_point(aes(x = date, y = resid))
```

## Model Selection
```{r m14_gamma}
lowermod <- glm("gpp ~ 1",
    family = Gamma(link = "log"),
    data = train %>% filter_all(all_vars(!is.na(.))))
modelsel <- MASS::stepAIC(lowermod,
                          scope = as.formula(paste0("~ 1 + (",
                          paste(pgnames, collapse = "+"),
                          ") * (",
                          paste0("log(",gppnames,")", collapse = " + "),
                          ") * (",
                          paste0(othernames, collapse = " + "),
                          ")")
                   ),
                   direction = "backward")

mod <- glm(paste0("gpp ~ ",paste(covariatenames, collapse = "+")),
    family = gaussian(link = "log"),
    data = train %>% filter_all(all_vars(!is.na(.))))  #this filter removes any rows with NA values
modelsel <- MASS::stepAIC(mod)


# mod <- glm(paste0("gpp ~ 1 + (",paste(pgnames, collapse = "+"),
#                                                     ") * (",
#                                                      paste0("log(",gppnames,")", collapse = " + "), ")"),
#     family = gaussian(link = "log"),
#     data = train %>% filter_all(all_vars(!is.na(.)))) 
# modelsel <- MASS::stepAIC(mod)
```


```{r m15_lognormal}
lowermod <- lm("log(gpp) ~ 1",
    data = train %>% filter_all(all_vars(!is.na(.))))
modelsel <- MASS::stepAIC(lowermod,
                          scope = as.formula(paste0("gpp ~ 1 + (",
                          paste(pgnames, collapse = "+"),
                          ") * (",
                          paste0("log(",gppnames,")", collapse = " + "),
                          ") * (",
                          paste0(othernames, collapse = " + "),
                          ")")
                   ),
                   direction = "forward")

mod <- glm(paste0("gpp ~ ",paste(covariatenames, collapse = "+")),
    family = gaussian(link = "log"),
    data = train %>% filter_all(all_vars(!is.na(.))))  #this filter removes any rows with NA values
modelsel <- MASS::stepAIC(mod)


# mod <- glm(paste0("gpp ~ 1 + (",paste(pgnames, collapse = "+"),
#                                                     ") * (",
#                                                      paste0("log(",gppnames,")", collapse = " + "), ")"),
#     family = gaussian(link = "log"),
#     data = train %>% filter_all(all_vars(!is.na(.)))) 
# modelsel <- MASS::stepAIC(mod)
```



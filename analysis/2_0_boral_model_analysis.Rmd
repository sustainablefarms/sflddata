---
title: "Analysis of Boral Models 3 and 4_1"
author: "Kassel Hingee"
date: "09/02/2020"
output: 
  html_document: 
    fig_height: 8
    fig_width: 10
    toc: yes
---

## Preparation
```{r setup, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE)
out <- lapply(c("sf", "tsibble", 'lubridate', "viridis",
                "boral",
                'ggplot2', 'tidyr', 'grid', 'gridExtra', 
                'feasts', 'dplyr', 'gtable', 'fable',
                'mgcv'),
       library, character.only = TRUE)
out <- lapply(paste0("../R/", list.files("../R/")), source)
```

```{r importdata}
birds <- readRDS("../private/data/clean/sws_birds.rds") # contains only common bird spp
sites_rs <- readRDS("../private/data/clean/sws_sites_rs.rds")
  sites_rs$log_plus_one_woody_cover <- log(sites_rs$woody_cover + 1)
traits <- readRDS("../private/data/clean/sws_traits.rds")
```

```{r importfittedmodels}
m3_0 <- readRDS("../private/models/boral_model_2019-12-18.rds")
m4_1 <- readRDS("../private/models/boral_model_2020-02-06_m1b_resid.rds")
```

## Model m3_0
```{r m3_0_anal1}
plot(m3_0)
```

The plots do not contradict the model assumptions:

+ quantiles match theoretical values
+ residual distribution independent of column index, row index (the residuals using the median of the posterior coefficient distributions)
+ residual distribution seems to change only slightly with the linear prediction


```{r m3_0_convergence}
#formal test as suggested by Boral authors. p-value limit of 0.01 would be nice (would be very inconvenient if the chain didn't converge)
gew.pvals <-2*pnorm(abs(unlist(m3_0$geweke.diag[[1]])),lower.tail = FALSE)
any(p.adjust(gew.pvals,method = "holm") < 0.01)
# test passed! The p-values are not inconsistent with the hypothesis the mcmc has converged

# Plot a few for sanity
mcmcsampls <- get.mcmcsamples(m3_0)
dim(mcmcsampls)
plot(mcmcsampls[ , sample(1:4799, 3)])
```

135 samples, 4799 parameters.

Column Names in the mcmc sample table above:

+ Coefficients of covariates (X) are the X.coefs (1 for each species)
+ Latent variables are likely abbreviated lvs and lv.coefs. Not sure why there are so many of them.
   + Latent variable value for the particular site could be lvs
   + lv.coefs could be the coefficients

### Visualise Predictions

#### By site and time
```{r m3_0_predictions_view}
# extract input data frame by only the rows used in the m3_0 model
na_check <- which(apply(
  cbind(sites_rs[, c("gpp_mean", "woody_cover", "fmc_diff", "gpp_diff")], birds),
  1,
  function(a){all(!is.na(a))}
))
m3_0$sites_rs <- sites_rs[na_check, ]
m3_0$birds <- birds[na_check, ]


# generate predictions
m3_0$preds_median <- predict.boral(m3_0, predict.type = "conditional",
             scale = "response") #prediction of probability of detection
#colidx <- which(colnames(m3_0$y) == "Australasian Pipit")
colidx <- 2
preddf <- cbind(
      Pred.med = m3_0$preds_median$linpred[, colidx],
      Pred.lower = m3_0$preds_median$lower[, colidx],
      Pred.upper = m3_0$preds_median$upper[, colidx],
      Obs = m3_0$y[, colidx],
      m3_0$sites_rs)

preddf %>%
  # filter(SiteCode == "BELL1") %>%
  ggplot() +
  geom_ribbon(aes(x = SurveyDate, ymin = Pred.lower, ymax = Pred.upper), fill = "grey70") +
  geom_line(aes(x = SurveyDate, y = Pred.med), col = "blue") +
  geom_point(aes(x = SurveyDate, y = Obs)) +
  ylab("Estimated Detection Probability / Observed Detections") +
  facet_wrap(vars(SiteCode)) +
  ggtitle(paste("m3_0 predictions of ", colnames(m3_0$y)[colidx]),
          subtitle = "95% credible intervals")
```

## Model m4_1
```{r m4_1_anal1}
plot(m4_1)
```


```{r m4_1_convergence}
#formal test as suggested by Boral authors. p-value limit of 0.01 would be nice (would be very inconvenient if the chain didn't converge)
gew.pvals <-2*pnorm(abs(unlist(m3_0$geweke.diag[[1]])),lower.tail = FALSE)
any(p.adjust(gew.pvals,method = "holm") < 0.01)
# test passed! The p-values are not inconsistent with the hypothesis the mcmc has converged

# Plot a few for sanity
mcmcsampls <- get.mcmcsamples(m3_0)
dim(mcmcsampls)
plot(mcmcsampls[ , sample(1:4799, 3)])
```

```{r m4_1_coefsplot}
# devtools::install_github("mjwestgate/boralis")
library(boralis)

## PLOT COEFFICIENTS
boral_data <- order_boral(boral_coefs(m4_1), "gpp_mean")
boral_coefsplot(boral_data) +
  # facet_wrap( # to manually override subplot arrangement
  #   facets = ~covname,
  #   # nrow = 1,
  #   # ncol = 4,
  #   scales = "free_x"
  # ) +
  theme(
    axis.text = element_text(size = 6),
    strip.background = element_rect(
      fill = "white",
      color = "white"),
    strip.text = element_text(
      hjust = 0,
      size = 8
    )
  )


## PLOT RESULTS FOR A SINGLE SPECIES
# "Superb Parrot", "Superb Fairy-wren"
boral_results <- boral_coefs(m4_1)
boral_results <- boral_results[which(boral_results$labels == "Superb Fairy-wren"), ]
boral_results <- boral_results[order(boral_results$x^2, decreasing = TRUE), ]
# factor_levels <- c("GPP (mean)", "GPP (variance)", "Date", "GPP * Date", "FMC (variance)", "Woody Veg. Cover") # SP
# factor_levels <- c("Woody Veg. Cover", "GPP (mean)", "GPP * Date", "GPP (variance)", "Date",  "FMC (variance)")
boral_results$cov_ordered <- factor(
  seq_len(nrow(boral_results)),
  levels = seq_len(nrow(boral_results)),
  labels = #boral_results$covname
)

ggplot(boral_results, aes(x = cov_ordered, y = x)) +
  geom_point() +
  geom_errorbar(aes(ymin = x0, ymax = x1), width = 0.5) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  theme_bw() +
  # ylim(-0.75, 0.25) + # SP
  xlab("Predictor Variable") +
  ylab("Model Coefficient") +
  ggtitle("Factors influencing occurrence of the Superb Fairy-wren") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



# plot results for a single variable
boral_data <- order_boral(boral_coefs(m4_1, "gpp_mean"), "gpp_mean")
boral_coefsplot(boral_data) +
  xlab("Coefficient of mean GPP") +
  theme(axis.text.y = element_text(size = 6))


# PLOT LATENT VARIABLES (ordination)
ordination_data <- as.data.frame(m4_1$lv.coefs.median)
ordination_data$taxon <- rownames(ordination_data)
rownames(ordination_data) <- NULL
ggplot(ordination_data, aes(x = theta1, y = theta2, label = taxon)) +
  geom_text(size = 3) +
  expand_limits(x = c(-1, 1))

# correlations
resid_cor <- get.residual.cor(m4_1)
# this is correlation that is due to the regression coefficients of the latent variables

enviro_cor <- get.enviro.cor(m4_1)
# the correlation in responses due to the explanotory variables (the predictors) - computed from regression coefficients too

# install.packages("corrplot")
library(corrplot)
corrplot(resid_cor$sig.cor, title = "Residual correlations",
  type = "lower", diag = FALSE, tl.srt = 45, tl.col = "grey30")

corrplot(enviro_cor$sig.cor, title = "Environmental correlations",
  type = "lower", diag = FALSE, tl.srt = 45, tl.col = "grey30")
```

### Visualise Predictions
#### By site and time
```{r m4_1_predictions_view}
# extract input data frame by only the rows used in the m3_0 model
na_check <- which(apply(
  cbind(sites_rs[, c("gpp_mean", "woody_cover", "m1b_resid")], birds),
  1,
  function(a){all(!is.na(a))}
))
m4_1$sites_rs <- sites_rs[na_check, ]
m4_1$birds <- birds[na_check, ]


# generate predictions
m4_1$preds_median <- predict.boral(m4_1, predict.type = "conditional",
             scale = "response") #prediction of probability of detection
#colidx <- which(colnames(m3_0$y) == "Australasian Pipit")
colidx <- 2
preddf <- cbind(
      Pred.med = m4_1$preds_median$linpred[, colidx],
      Pred.lower = m4_1$preds_median$lower[, colidx],
      Pred.upper = m4_1$preds_median$upper[, colidx],
      Obs = m4_1$y[, colidx],
      m4_1$sites_rs)

preddf %>%
  # filter(SiteCode == "BELL1") %>%
  ggplot() +
  geom_ribbon(aes(x = SurveyDate, ymin = Pred.lower, ymax = Pred.upper), fill = "grey70") +
  geom_line(aes(x = SurveyDate, y = Pred.med), col = "blue") +
  geom_point(aes(x = SurveyDate, y = Obs)) +
  ylab("Estimated Detection Probability / Observed Detections") +
  facet_wrap(vars(SiteCode)) +
  ggtitle(label = paste("m4_1 predictions of", colnames(m4_1$y)[colidx]),
          subtitle = "95% credible intervals")
```

## Compare Predictive Quality of the Models
```{r comparingmodels1_bic}
m3_0$condloglik <- calc.condlogLik(m3_0$y, X = m3_0$X, family = "binomial",
                lv.coefs = m3_0$lv.coefs.median, X.coefs = m3_0$X.coefs.median,
                lv = m3_0$lv.median)

m4_1$condloglik <- calc.condlogLik(m4_1$y, X = m4_1$X, family = "binomial",
                lv.coefs = m4_1$lv.coefs.median, X.coefs = m4_1$X.coefs.median,
                lv = m4_1$lv.median)

m3_0$condloglik[[1]]
m4_1$condloglik[[1]]
k.boral <- function(m){ #not sure if this is correct! So not much point in using it
  k <-  length(m$X.coefs.median) +
    length(m$lv.coefs.median) +
    length(m$lv.median)
  return(k)
}
k.boral(m4_1)
k.boral(m3_0)

bic.boral <- function(m){
  n <- nrow(m$y)
  k <- k.boral(m)
  lL <- calc.condlogLik(m$y, X = m$X, family = "binomial",
                lv.coefs = m$lv.coefs.median, X.coefs = m$X.coefs.median,
                lv = m$lv.median)[[1]]
  bic <- log(n) * k - 2 * lL
  return(bic)
}
bic.boral(m3_0)
bic.boral(m4_1)
```

Higher log-likelihood means a better fit of parameters for a *given* model. However, the models aren't equal. Model m4_1 has fewer explanatory variables, so it might do better.

The BIC of m3_0 appears better (assuming I'm interpreting the definition of BIC properly).

```{r predictionerror}
m3_0$preds_median <- predict.boral(m3_0, predict.type = "conditional",
             scale = "response") #prediction of probability of detection
m4_1$preds_median <- predict.boral(m4_1, predict.type = "conditional",
             scale = "response")

summary(m4_1$y - m4_1$preds_median$linpred)
```

```{r visualise_predictions_of_both}
colidx <- which(colnames(m3_0$y) == "Noisy Miner")
# colidx <- 14
preddf_m3_0 <- cbind(
      m3_0.Pred.med = m3_0$preds_median$linpred[, colidx],
      m3_0.Pred.lower = m3_0$preds_median$lower[, colidx],
      m3_0.Pred.upper = m3_0$preds_median$upper[, colidx],
      Obs = m3_0$y[, colidx],
      m3_0$sites_rs)

preddf_m4_1 <- cbind(
      m4_1.Pred.med = m4_1$preds_median$linpred[, colidx],
      m4_1.Pred.lower = m4_1$preds_median$lower[, colidx],
      m4_1.Pred.upper = m4_1$preds_median$upper[, colidx],
      Obs = m4_1$y[, colidx],
      m4_1$sites_rs)

preddf <- inner_join(preddf_m3_0, preddf_m4_1)
preddf %>%
  # filter(SiteCode == "BELL1") %>%
  ggplot() +
  geom_ribbon(aes(x = SurveyDate, ymin = m3_0.Pred.lower, ymax = m3_0.Pred.upper), fill = "lightgreen", alpha = 1) +
  geom_ribbon(aes(x = SurveyDate, ymin = m4_1.Pred.lower, ymax = m4_1.Pred.upper), fill = "red", alpha = 0.2) +
  geom_line(aes(x = SurveyDate, y = m3_0.Pred.med), col = "darkgreen") +
  geom_line(aes(x = SurveyDate, y = m4_1.Pred.med), col = "red") +
  geom_point(aes(x = SurveyDate, y = Obs)) +
  ylab("Estimated Detection Probability / Observed Detections") +
  facet_wrap(vars(SiteCode)) +
  ggtitle(paste("Predictions of", colnames(m3_0$y)[colidx]),
          subtitle = "95% credible intervals")
```

So far there is very little difference between the predictions from each model. The predictions are pretty good considering how little information is put in!

A few sites (SCHU1) are pretty poor for the Common Starling.
Noisy Miner prediction is also poor for some sites (FEUE3).

It would be very nice to make this, and other diagnostics, into a shiny app if we continue to use Boral.
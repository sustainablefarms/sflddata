---
title: 'Modelling GPP: gams and interactions'
author: "Kassel Hingee"
date: "16/01/2020"
output: 
  html_document: 
    toc: yes
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE)
out <- lapply(c("sf", "tsibble", 'lubridate', "viridis",
                'ggplot2', 'tidyr', 'grid', 'gridExtra', 
                'feasts', 'dplyr', 'gtable', 'fable',
                'mgcv'),
       library, character.only = TRUE)
out <- lapply(paste0("../R/", list.files("../R/")), source)
```

*Proposal:* high rate of change in GPP will correspond to spikes. For gpp sufficiently high, grazing is increased to maximise profits and the GPP increase is slower. This suggests a model for the rate of GPP change as a function of current/previous GPP, day of the year, and recent rainfall, with interactions between all of these.

+ high GPP ==> low rate of GPP increase regardless of other covariates
+ high rainfall and low GPP ==> high rate of GPP increase
+ moderate rainfall, spring and low GPP ==> moderate rate of GPP increase

Modelling Paradigm:
Response is: GPP, not seasonally corrected.
Predictors:

1. last years GPP or average GPP
2. most recent GPP
3. cumulative rainfall at 1d, 7d, 15d, 1m, 2m, 3m, 5m, 8m, 14m to avoid having to include interactions between rainfall terms. Or use `mgcv` method for smoothing fitting with lags as per distributed lag models in Section 7.4.2 of Wood's book on `mgcv`.
4. day of the year
5. farm


```{r preparedata}
#load data and convert to tsibbles
load("../private/data/remote_sensed/pg_daily.Rdata")
pg_daily$times <- as_date(pg_daily$times)
pg <- pg_daily %>%
  pivot_longer(-times, names_to = "site", values_to = "pg") %>%
  as_tsibble(key = site, index = times)
load("../private/data/remote_sensed/gpp_8d.Rdata")
gpp <- gpp_8d %>%
  pivot_longer(-times, names_to = "site", values_to = "gpp") %>%
  as_tsibble(key = site, index = times)
pggpp <- as_tsibble(dplyr::full_join(pg, gpp, by = c("times", "site")), key = site, index = times)

# add times breakdowns
pggpp <- pggpp %>%
  mutate(yday = yday(times),
         year = year(times))
  

#interpolate gpp
pggpp <- pggpp %>%
  group_by_key() %>% #key is site
  mutate(lininterp_gpp = zoo::na.approx(gpp, rule = 2, na.rm = FALSE)) %>% #rule 2 means edges are assigned last value
  ungroup()

#make sure sites are ordered alphabetically
pggpp <- pggpp %>%
  arrange(site) %>%
  mutate(site = factor(site, ordered = TRUE))

#separate alpha part of site code
pggpp <- pggpp %>%
  mutate(farm = factor(substr(site, 1, 4)),
         sitenum = factor(as.integer(substr(site, 5, 5))))


# Add cumulative rainfalls
pggpp <- pggpp %>% 
  group_by_key() %>%
  mutate(pg_cumsum = cumsum(pg)) %>%
  mutate(pg_1to7 = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 8), # 1 up to 8 days behind (excluding 8th day)
         pg_1to15 = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 16), # 1 to 16 days behind
         pg_1to1m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 31),
         pg_1to2m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 2*31),
         pg_1to3m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 3*31),
         pg_1to4m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 4*31),
         pg_1to5m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 5*31),
         pg_1to6m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 6*31),
         pg_1to7m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 7*31),
         pg_1to8m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 8*31),
         pg_1to10m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 10*31),
         pg_1to12m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 12*31),
         pg_1to14m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 14*31),
         ) %>%
  ungroup()

# simple mean gpp of training years
ydaymeangpp <- pggpp %>%
  filter(yday %in% seq(1, 366, by = 8)) %>%
  group_by(site) %>%
  index_by(yday) %>%
  summarise(gpp.ydaymean = mean(gpp))
pggpp <- left_join(pggpp, ydaymeangpp, by = c("site", "yday"))
```

```{r preparetraintest}
train <- pggpp %>%
  filter(yday %in% seq(1, 366, by = 8)) %>%
  filter(sitenum == 1) %>% #so not doubling up at farms
  filter_index(. ~ "2016-12-31") %>%
  select(-pg_cumsum, -lininterp_gpp) %>%
  mutate(gpp = if_else(gpp < 0.1, as.double(NA), gpp)) #remove outlying GPP values that are super low

test <- pggpp %>%
  filter_index("2017-01-01" ~ .) %>%
  filter(sitenum == 1) %>% #so not doubling up at farms
  filter(yday %in% seq(1, 366, by = 8)) %>%
  select(-pg_cumsum)
```


In the above very small GPP values have been removed. There was `r sum(is.na(train$gpp))` of them, which corresponds to `r sum(is.na(train$gpp)) / length(train$gpp)` of the data.

## Model Fit of GAM with interactions between precipitation and seasonal gpp.
In first attempts to fit model, fitting the whole data set at once did not complete. In the following I try to fit the same model, but separate model for each farm. Hopefully the parameters of each model for each farm will be comparable.

First is a periodic gam with log link and using yday as a substitute for seasonal GPP.
```{r m1}
m1_bell1 <- gam(gpp ~  gpp.ydaymean +
                 #s(yday, bs = "cc", k = 20) + #substitute for using seasonal gpp
                 s(pg, gpp.ydaymean) +
                 s(pg_1to7, gpp.ydaymean) + 
                 s(pg_1to15, gpp.ydaymean) +
                 s(pg_1to1m, gpp.ydaymean) +
                 s(pg_1to2m, gpp.ydaymean) +
                 s(pg_1to3m, gpp.ydaymean) +
                 s(pg_1to5m, gpp.ydaymean) +
                 s(pg_1to8m, gpp.ydaymean) +
                 s(pg_1to14m, gpp.ydaymean)
                 ,
             family = gaussian(link = "log"),
             data = train %>% filter(site == "BELL1"))
train %>%
  filter(site == "BELL1") %>%
  tibble::rownames_to_column() %>%
  left_join(tibble::enframe(predict(m1_bell1, type = "response"), name = "rowname", value = "gpp.fitted"),
            by = "rowname") %>%
  filter_index("2005" ~ .) %>%
  pivot_longer(c("gpp.fitted", "gpp")) %>%
  ggplot() +
  geom_line(aes(x = times, y = gpp.ydaymean), alpha = 0.3) +
  geom_line(aes(x = times, y = value, col = name, lty = name), size = 0.5) +
  geom_point(aes(x = times, y = value, col = name, shape = name), size = 1) +
  ggtitle("m1 for bell1: fitted values") +
  scale_color_viridis_d()

plot(m1_bell1, scheme = 2)
m1_bell1
gam.check(m1_bell1)
```


The model predictions are better than the linear fits I did previously. The peaks were not modelled well initially and interpretation was obfuscated by the lag(gpp) use of seasonal gpp; here the seasonal gpp is using the mean across all training years instead.

The smooths of interactions between gpp.ydaymean and the cumulative rainfall have contours that run nearly vertical, suggesting that there was not much interaction between gpp.ydaymean and the cumulative rainfalls.

The residuals are not at all i.i.d. There appears to be quite a sharp striation of residuals in the resdiual vs linear predictor plot.

A lot of the edf vales are very close to k'. I think this is reason to suspect something.


The following uses `bam()`, which is like gam but designed for larger amounts of data.
```{r m1_allfarms}
m1 <- bam(gpp ~  gpp.ydaymean +
                 farm +
                 ti(pg_1to5m) + #ti so that interaction terms can be assessed.
                 #s(yday, bs = "cc", k = 20) + #substitute for using seasonal gpp
                 #s(pg, gpp.ydaymean) +
                 #s(pg_1to7, gpp.ydaymean) + 
                 #s(pg_1to15, gpp.ydaymean) +
                 #s(pg_1to1m, gpp.ydaymean) +
                 # ti(pg_1to2m, gpp.ydaymean) +  #main effects  could be quite complex
                 # ti(pg_1to2m, gpp.ydaymean, by = farm, id = "2") + #difference effects, hopefully simple
                 #s(pg_1to3m, gpp.ydaymean) +
                 ti(pg_1to5m, gpp.ydaymean) + #the ti() means non-isotropic smoothing and interaction-only terms, id = "a" means each smooth for each factor level has the same smoothing penalty, basis functions, knots etc.
                 ti(pg_1to5m, gpp.ydaymean, by = farm, id = "a")  #the ti() means non-isotropic smoothing and interaction-only terms, id = "a" means each smooth for each factor level has the same smoothing penalty, basis functions, knots etc.
                 #s(pg_1to14m, gpp.ydaymean)
                 ,
             family = gaussian(link = "log"),
             # use AR.start and rho for autocorrelated errors
             data = train %>% filter(sitenum == 1))
#vis.gam(m1)
plot(m1)
gam.check(m1)
summary(m1)

train %>%
  filter(sitenum == 1) %>%
  tibble::rownames_to_column() %>%
  left_join(tibble::enframe(predict(m1, type = "response"), name = "rowname", value = "gpp.fitted"),
            by = "rowname") %>%
  filter(farm == "BELL") %>%
  filter_index("2005" ~ .) %>%
  pivot_longer(c("gpp.fitted", "gpp")) %>%
  ggplot() +
  geom_line(aes(x = times, y = gpp.ydaymean), alpha = 0.3) +
  geom_line(aes(x = times, y = value, col = name, lty = name), size = 0.5) +
  geom_point(aes(x = times, y = value, col = name, shape = name), size = 1) +
  ggtitle("m1: fitted values at BELL1") +
  scale_color_viridis_d()
```

The above model takes a long time to fit. the Q-Q plot and residual vs linear predictor plot show that the distributional assumptions are wrong. The residuals appears skewed and their standard deviation appears grow exponentially (this could help find the right link function).

Predictions appear to be as poor as all the others models.

The model has intercepts and 16 interaction terms (one for each basis function perhaps?) for each farm.
```{r m1_coefficientsforfarms, width = 15, height = 15}
o <- coef(m1)
o2 <- o %>% 
  as_tibble(rownames = "coefname") %>%
  filter(grepl("farm", coefname)) %>%
  mutate(termshort = gsub("farm....", "", coefname)) %>%
  mutate(termshort = if_else(termshort == "", "Intercept", termshort)) %>%
  #mutate(termshort = gsub("ti(pg_1to5m,gpp.ydaymean):", "pg_5m:gpp_s", termshort)) %>%
  mutate(farm = substring(coefname, regexpr("farm", coefname) + 4, regexpr("farm....", coefname) +  7))  %>%
  select(-coefname) 
o2 %>%
  inner_join(o2 %>% filter(termshort == "Intercept") %>% select("farm", "value"), by = "farm", suffix = c("", ".Intercept")) %>%
  rename(Intercept = value.Intercept) %>%
  ggplot() +
  facet_wrap(~termshort, scales = "free") +
  geom_point(aes(x = Intercept, y = value, col = farm, shape = farm)) +
  # geom_text_repel(aes(x = Intercept, y = value, label = farm),
  #                 data = function(x) sample_frac(x, size = 0.1),
  #                 size = 2) +
  scale_color_viridis_d() +
  scale_shape_manual(values = rep(0:15, 5)) +
  ggtitle("The Coefficient for Each Farm Term in Model 1")
```

Consistent with my finding in exploration, GEDD has some outlying properties.


## Fit a GAM with autocorrelated errors
```{r m2, eval = FALSE}
m2_bell1 <- gamm(gpp ~  gpp.ydaymean +
                 #s(yday, bs = "cc", k = 20) + #substitute for using seasonal gpp
                 # s(pg, gpp.ydaymean) +
                 # s(pg_1to7, gpp.ydaymean) + 
                 # s(pg_1to15, gpp.ydaymean) +
                 # s(pg_1to1m, gpp.ydaymean) +
                 # s(pg_1to2m, gpp.ydaymean) +
                 s(pg_1to3m, gpp.ydaymean) +
                 s(pg_1to5m, gpp.ydaymean) 
                 # s(pg_1to8m, gpp.ydaymean) +
                 # s(pg_1to14m, gpp.ydaymean)
                 ,
             family = gaussian(link = "log"),
             correlation = corARMA(p = 1, q = 1, form = ~times),
             data = train %>% filter(site == "BELL1"))
```

Fitting attempts fo the above failed due to 'Singularity in backsolve at level 0, block 1' in the MEestimate function.
Do not know what is causing this, or how to avoid it.

## Fit a GAMM without correlated errors
### Using `gamm` and the 'random' parameter
I've chosen to use just pg_1to5m as this and pg_1to8m were the only rainfall covariates that had an (untrustworthy anyway) small $p$ value.

```{r m3}
m3 <- gamm(gpp ~  s(pg_1to5m),
             random = list(site = ~1 + pg_1to5m),
             family = gaussian(link = "log"),
             data = train %>% filter(sitenum == 1, !is.na(pg_1to5m)) )
plot(m3$gam)
plot(m3$lme)
ranef(m3$lme)
m3
plot(m3$lme, site ~ resid(.))
```

A model was fitted successfully, but I've no idea how to assess the model quality.
There are a lot of residuals outside the usual band in the site vs residuals plot.
The smooth fit for pg_1to5m seems appropriate, shape: the impact of rain flattens out after 400mm (which is an incredible amount of rain).

What is the 'g'-level random effect? A: related to the smoothing

*Why do the site codes have '1' in front of them?* 
*what does the warning about non-contrast terms mean?*

I'm also not entirely sure if the random effect is acting on the spline or on a separate linear term. Below is the prediction as if the random effect is action on separate linear terms. 

The package `mgcv` can take random effects in `gam()` through `s()` with basis `bs="re"`, need to look into this more.

```{r m3_assess_predictions}
pred <- train %>% filter(sitenum == 1, !is.na(pg_1to5m)) %>% tibble::rownames_to_column(var = "rowname")
pred <- pred %>%
  left_join(tibble::enframe(predict(m3$gam, type = "response"), name = "rowname", value = "gam.predict"),
            by = "rowname")
pred %>%
  filter(site == "BELL1") %>%
  mutate(gamm.predict = gam.predict + sum(ranef(m3$lme, level = 2)["1/BELL1", ] * c(1, pg_1to5m)) ) %>%
  pivot_longer(c(gamm.predict, gam.predict, gpp)) %>%
  ggplot() +
  geom_point(aes(x = times, y = value, col = name, shape = name))
```

Either I'm misunderstanding the model and computing predictions incorrectly, or the model is fairly poor for BELL (see above figure). __Have I got the predictions correct?__ If so then I can't see how the random intercepts below have any meaning.

```{r m3_assessment_ranintercepts}
library(ggrepel) # for adding text labels at nice locations
ranef(m3$lme, level = 2) %>%
  as_tibble(rownames = "site") %>%
  ggplot() +
  geom_point(aes(x = `(Intercept)`, y = pg_1to5m)) +
  geom_text_repel(aes(x = `(Intercept)`, y = pg_1to5m, label = gsub("1/", "", site)),
              inherit.aes = TRUE,
              size = 3)
```

### Using `gam` and the the 're' basis
```{r m8}
filtertrain <- function(x){
  x <- x %>% filter(sitenum == 1,
  #farm %in% c("BELL", "WILS", "GEDD"),
  !is.na(pg_1to14m),
  !is.na(gpp))
  return(x)
}
  
trainsp <- train %>% filtertrain()
cumpg <- select(as_tibble(trainsp), c(pg_1to7,
                         pg_1to15 ,
                         pg_1to1m ,
                         pg_1to2m ,
                         pg_1to3m ,
                         pg_1to4m ,
                         pg_1to5m ,
                         pg_1to6m ,
                         pg_1to7m ,
                         pg_1to8m ,
                         pg_1to10m,
                         pg_1to12m,
                         pg_1to14m)) %>% as.matrix()
lagm <- matrix(c(1:ncol(cumpg)), ncol = ncol(cumpg), nrow = nrow(cumpg), byrow = TRUE)


m8 <- gam(box_cox(gpp / gpp.ydaymean, lambda = 0.2626) ~ 
              te(cumpg, lagm, id = "b") +
              site +
              s(pg_1to5m, bs = "re", by = site),
             family = gaussian(link = "log"),
             data = train %>% filtertrain() )
plot(m8, rug = TRUE)
gam.check(m8)
summary(m8)

pred <- train %>% 
  filtertrain() %>%
  tibble::rownames_to_column(var = "rowname")
pred <- pred %>%
  left_join(tibble::enframe(predict(m8, type = "response"), name = "rowname", value = "gam.linpredict"),
            by = "rowname") %>%
  mutate(gam.predict = inv_box_cox(gam.linpredict, lambda = 0.2626) * gpp.ydaymean)
pred %>%
  filter(farm == "WILS") %>%
  pivot_longer(c(gam.predict, gpp)) %>%
  ggplot() +
  geom_line(aes(x = times, y = gpp.ydaymean), col = "black", size = 0.5) +
  geom_line(aes(x = times, y = value, col = name)) +
  geom_point(aes(x = times, y = value, col = name, shape = name))
```

The predictions are almost entirely right on the mean gpp. Exceptions are peaks.
Distribution of residuals is better, but variance is not constant.

### Fit same model without site-specific effects.
```{r m9}
filtertrain <- function(x){
  x <- x %>% filter(sitenum == 1,
  #farm %in% c("BELL", "WILS", "GEDD"),
  !is.na(pg_1to14m),
  !is.na(gpp))
  return(x)
}
  
trainsp <- train %>% filtertrain()
cumpg <- select(as_tibble(trainsp), c(pg_1to7,
                         pg_1to15 ,
                         pg_1to1m ,
                         pg_1to2m ,
                         pg_1to3m ,
                         pg_1to4m ,
                         pg_1to5m ,
                         pg_1to6m ,
                         pg_1to7m ,
                         pg_1to8m ,
                         pg_1to10m,
                         pg_1to12m,
                         pg_1to14m)) %>% as.matrix()
lagm <- matrix(c(1:ncol(cumpg)), ncol = ncol(cumpg), nrow = nrow(cumpg), byrow = TRUE)


m9 <- gam(box_cox(gpp / gpp.ydaymean, lambda = 0.2626) ~ 
              te(cumpg, lagm, id = "b") +
              s(pg_1to5m, bs = "re"),
             family = gaussian(link = "log"),
             data = train %>% filtertrain() )
plot(m9)
gam.check(m9)
summary(m9)

pred <- train %>% 
  filtertrain() %>%
  tibble::rownames_to_column(var = "rowname") %>%
  left_join(tibble::enframe(predict(m9, type = "response"), name = "rowname", value = "gam.linpredict_m9"),
            by = "rowname") %>%
  left_join(tibble::enframe(predict(m9, type = "response", exclude = "te(cumpg,lagm)"), name = "rowname", value = "gam.linpredict_m9_s_only"),
            by = "rowname") %>%
  left_join(tibble::enframe(predict(m9, type = "response", exclude = 's(pg_1to5m)'), name = "rowname", value = "gam.linpredict_m9_te_only"),
            by = "rowname") %>%
  left_join(tibble::enframe(predict(m8, type = "response"), name = "rowname", value = "gam.linpredict_m8"),
            by = "rowname") %>%
  mutate(gam.predict_m8 = inv_box_cox(gam.linpredict_m8, lambda = 0.2626) * gpp.ydaymean) %>%
  mutate(gam.predict_m9 = inv_box_cox(gam.linpredict_m9, lambda = 0.2626) * gpp.ydaymean) %>%
  mutate(gam.predict_m9_s_only = inv_box_cox(gam.linpredict_m9_s_only, lambda = 0.2626) * gpp.ydaymean) %>%
  mutate(gam.predict_m9_te_only = inv_box_cox(gam.linpredict_m9_te_only, lambda = 0.2626) * gpp.ydaymean)
pred %>%
  filter(farm == "WILS") %>%
  pivot_longer(c(gam.predict_m8, gam.predict_m9, gam.predict_m9_s_only, gam.predict_m9_te_only, gpp)) %>%
  ggplot() +
  geom_line(aes(x = times, y = gpp.ydaymean), col = "black", size = 0.5) +
  geom_line(aes(x = times, y = value, col = name, lty = name)) +
  #geom_point(aes(x = times, y = value, col = name, shape = name)) +
  ggtitle("m9: m8 without site specific effects")
```

There is very little difference between predictions from the model with site specific effects (m8) and the model without (m9). A possible course of action will be to look at the residuals from model m9 for each site. 

## Fit GAM where Smooths are smooth across lags
Take One:
```{r m5}
trainsp <- train %>% filter(sitenum == 1, !is.na(pg_1to14m), !is.na(gpp))
cumpg <- select(as_tibble(trainsp), c(pg_1to7,
                         pg_1to15 ,
                         pg_1to1m ,
                         pg_1to2m ,
                         pg_1to3m ,
                         pg_1to4m ,
                         pg_1to5m ,
                         pg_1to6m ,
                         pg_1to7m ,
                         pg_1to8m ,
                         pg_1to10m,
                         pg_1to12m,
                         pg_1to14m)) %>% as.matrix()
lagm <- matrix(c(1:ncol(cumpg)), ncol = ncol(cumpg), nrow = nrow(cumpg), byrow = TRUE)

m5 <- bam(gpp ~ gpp.ydaymean + 
            s(yday, by = farm, id = "a") +
            te(cumpg, lagm, id = "b"),
            family = gaussian(link = "log"),
            data = trainsp
            )
```

Fitting of the above was quite fast!

```{r m5_plot}
plot(m5)
gam.check(m5)

preds <- predict(m5, type = "response", se.fit = TRUE) 
pred <- preds %>%
  as_tibble() %>%
  tibble::rownames_to_column()

trainsp %>%
  tibble::rownames_to_column() %>%
  left_join(pred,
            by = "rowname") %>%
  filter(farm == "WILS") %>%
  filter_index("2005" ~ .) %>%
  ggplot() +
  geom_ribbon(aes(x = times, ymin = fit - 2 *se.fit, ymax = fit + 2 * se.fit), col = "grey") +
  geom_line(aes(x = times, y = fit)) +
  geom_line(aes(x = times, y = gpp), col = "blue", size = 0.5) +
  geom_line(aes(x = times, y = gpp.ydaymean), alpha = 0.3, lty = "dashed") +
  ggtitle("m1: fitted values at BELL1") +
  scale_color_viridis_d()
```

The residuals, like all previous models, are not distributed consistent with the assumptions. The predictions with the 2*se are as poor as all previous models.

__I'm not sure if my use of the cumulative rainfal has behaved in the way intended.__ Is it possible to tell?

Autocorrelatted errors cannot be included when using `bam()` as it requires the family to be `gaussian(link = "identity")`. So fit with log(GPP + 1) instead, which means log(GPP +1) - mean is assumed to be Gaussian.

## Fit GAM of log(GPP + 1) with correlated errors.
```{r m6}
trainsp <- train %>% filter(sitenum == 1, !is.na(pg_1to14m), !is.na(gpp))
cumpg <- select(as_tibble(trainsp), c(pg_1to7,
                         pg_1to15 ,
                         pg_1to1m ,
                         pg_1to2m ,
                         pg_1to3m ,
                         pg_1to4m ,
                         pg_1to5m ,
                         pg_1to6m ,
                         pg_1to7m ,
                         pg_1to8m ,
                         pg_1to10m,
                         pg_1to12m,
                         pg_1to14m)) %>% as.matrix()
lagm <- matrix(c(1:ncol(cumpg)), ncol = ncol(cumpg), nrow = nrow(cumpg), byrow = TRUE)

m6 <- bam(log(gpp + 1) ~ log(gpp.ydaymean + 1) + 
            s(yday, by = farm, id = "a") +
            te(cumpg, lagm, id = "b"),
            family = gaussian(link = "identity"),
            rho = 0.9,
            data = trainsp
            )

gam.check(m6)
plot(m6)

preds <- predict(m6, type = "response", se.fit = TRUE) 
pred <- preds %>%
  as_tibble() %>%
  tibble::rownames_to_column()

trainsp %>%
  tibble::rownames_to_column() %>%
  left_join(pred,
            by = "rowname") %>%
  filter(farm %in% c("BELL", "WILS")) %>%
  filter_index("2005" ~ .) %>%
  ggplot() +
  facet_grid(rows = vars(farm)) +
  geom_ribbon(aes(x = times, ymin = exp(fit - 2 *se.fit) - 1 ,
                  ymax = exp(fit + 2 * se.fit) - 1), col = "grey") +
  geom_line(aes(x = times, y = exp(fit) - 1)) +
  geom_line(aes(x = times, y = gpp), col = "blue", size = 0.5) +
  geom_line(aes(x = times, y = gpp.ydaymean), alpha = 0.3, lty = "dashed") +
  ggtitle("m1: fitted values at WILS and BELL") +
  scale_color_viridis_d()
```

The model m6 with high autocorrelation fitted really fast!

Gam Check:
The QQ plot of residuals vs theoretical quantiles and the residuals vs linear predictor are not not quite right, but they seem to be an improvement. Striations of response values are really obvious in response vs fitted values. *Have GPP values been binned or rounded?*

Fitted smooths: 
the relation between yday and GPP, factored by farm, was virtually zero. This makes sense given the mean GPP for each farm for each yday is already included.
The relation between cumulative pg and lag is also nearly zero. That worries me too.

Predictions: For BELL and WILS predictions of peaks appears to be as accuracte as `m1`. Troughs seem to do better though.

## Fit a GAM with correlated errors for BoxCox(GPP/meanGPP)
```{r m10}
filtertrain <- function(x){
  x <- x %>% filter(sitenum == 1,
                    farm %in% c("BELL", "WILS"),
                    !is.na(pg_1to14m),
                    !is.na(gpp))
  return(x)
}
  
trainsp <- train %>% filtertrain()
cumpg <- select(as_tibble(trainsp), c(pg_1to7,
                         pg_1to15 ,
                         pg_1to1m ,
                         pg_1to2m ,
                         pg_1to3m ,
                         pg_1to4m ,
                         pg_1to5m ,
                         pg_1to6m ,
                         pg_1to7m ,
                         pg_1to8m ,
                         pg_1to10m,
                         pg_1to12m,
                         pg_1to14m)) %>% as.matrix()
lagm <- matrix(c(1:ncol(cumpg)), ncol = ncol(cumpg), nrow = nrow(cumpg), byrow = TRUE)


m10 <- gam(box_cox(gpp / gpp.ydaymean, lambda = 0.2626) ~ 1 +
             te(cumpg, lagm),
             family = gaussian(link = "log"),
             #correlation = corAR1(0.9, form = ~times | farm, fixed = FALSE),
             data = train %>% filtertrain() )
```


## Fit an ARIMA for GPP
```{r m7_arima}
trainsp <- train %>%
  filter(yday %in% seq(1, 366, by = 8)) %>%
  tibble::rowid_to_column() %>% #reindex by approximate 8 day intervals (discrepancies at the end of every year)
  as_tsibble(key = "site", index = "rowid") %>%
  group_by_key()


### The following was erroring: no max values to choose from, no appropriate ARIMA model found...
# m7sp <- fable::ARIMA(log(gpp + 1) ~ 1 + pdq(0:3, 1,0:3) + PDQ(7, 1, 10, period = 46), order_constraint = p + q + P +
#   Q <= 40)
# m7 <- trainsp %>%
#   filter(site == "BELL1") %>%
#   mutate(gpp = if_else(is.na(gpp),
#                        0.1,
#                        gpp)) %>%
#   select(gpp) %>%
#   fabletools::model(m7sp)
#####################

m7 <- stats::arima((trainsp %>% filter(site == "BELL1"))$gpp, 
             order = c(3, 1, 3),
             seasonal = list(order = c(7, 1, 7), period = 46))
             


fitted(m7)
```

The maximum lag is 350, so in above made Q = 7.


## Fit a GAM with Smooths across lag, no sites.
```{r m11}
filtertrain <- function(x){
  x <- x %>% filter(sitenum == 1,
                    #farm %in% c("BELL", "WILS", "GEDD"),
                    !is.na(pg_1to14m),
                    yday %in% seq(1, 366, by = 3* 8), #reduce correlation in data fitting and reduce fitting time
                    !is.na(gpp))
  return(x)
}
  
trainsp <- train %>% filtertrain()
cumpg <- select(as_tibble(trainsp), c(pg_1to7,
                         pg_1to15 ,
                         pg_1to1m ,
                         pg_1to2m ,
                         pg_1to3m ,
                         pg_1to4m ,
                         pg_1to5m ,
                         pg_1to6m ,
                         pg_1to7m ,
                         pg_1to8m ,
                         pg_1to10m,
                         pg_1to12m,
                         pg_1to14m)) %>% as.matrix()
lagm <- matrix(c(1:ncol(cumpg)), ncol = ncol(cumpg), nrow = nrow(cumpg), byrow = TRUE)


m11 <- gam(box_cox(gpp / gpp.ydaymean, lambda = 0.2626) ~ 
              te(cumpg, lagm, id = "b"),
              family = gaussian(link = "log"),
              data = train %>% filtertrain() )

gam.check(m11)

pred <- train %>% 
  filtertrain() %>%
  tibble::rownames_to_column(var = "rowname") %>%
  left_join(tibble::enframe(predict(m11, type = "response"), name = "rowname", value = "gam.linpredict_m11"),
            by = "rowname") %>%
  mutate(gam.predict_m11 = inv_box_cox(gam.linpredict_m11, lambda = 0.2626) * gpp.ydaymean) 
pred %>%
  filter(farm == "WILS") %>%
  pivot_longer(c(gam.predict_m11, gpp)) %>%
  ggplot() +
  geom_line(aes(x = times, y = gpp.ydaymean), col = "black", size = 0.5) +
  geom_line(aes(x = times, y = value, col = name, lty = name)) +
  #geom_point(aes(x = times, y = value, col = name, shape = name)) +
  ggtitle("m11: just te term")
```

Residuals of this model still do not fit assumptions, but it does better than the pure yday mean of GPP.

The explain variation (R2 value) for m11 is also very low (0.2).

```{r m11_proporionalresiduals}
pred %>%
  mutate(propresid = gam.predict_m11/ gpp) %>%
  ggplot() +
  facet_wrap(~farm) +
  geom_hline(yintercept = 1, col = "grey") +
  geom_line(aes(x = times, y = propresid)) +
  #geom_point(aes(x = times, y = value, col = name, shape = name)) +
  ggtitle("m11 (just te term)") +
  scale_y_continuous(name = "predicted gpp / observed gpp",
                     trans = "log10")

```

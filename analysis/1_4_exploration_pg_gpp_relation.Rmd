---
title: "More Exploration of GPP and precipitation"
author: "Kassel Hingee"
date: "13/01/2020"
output: html_document
---

The following exploration occurs after fitting an additive linear model, and log-linear model for GPP. It uses learnings summarised in `1_3_3_model_fitting_pg_gpp.Rmd`.

+ correlation analysis, a variety of transforms and derivative precipitation infomration, would be useful for choosing predictors
+ using pre-built decomposition methods within R to explore break down too
+ exploring stationarity of GPP


```{r setup}
knitr::opts_chunk$set(echo = TRUE)
out <- lapply(c("sf", "tsibble", 'lubridate', "viridis",
                'ggplot2', 'tidyr', 'grid', 'gridExtra', 
                'feasts', 'dplyr', 'gtable', 'fable'),
       library, character.only = TRUE)
out <- lapply(paste0("../functions/", list.files("../functions/")), source)
```

```{r preparedata}
#load data and convert to tsibbles
load("../private/data/remote_sensed/pg_daily.Rdata")
pg_daily$times <- as_date(pg_daily$times)
pg <- pg_daily %>%
  pivot_longer(-times, names_to = "site", values_to = "pg") %>%
  as_tsibble(key = site, index = times)
load("../private/data/remote_sensed/gpp_8d.Rdata")
gpp <- gpp_8d %>%
  pivot_longer(-times, names_to = "site", values_to = "gpp") %>%
  as_tsibble(key = site, index = times)
pggpp <- as_tsibble(dplyr::full_join(pg, gpp, by = c("times", "site")), key = site, index = times)

#interpolate gpp
pggpp <- pggpp %>%
  mutate(yday = yday(times)) %>%
  group_by_key() %>% #key is site
  mutate(lininterp_gpp = zoo::na.approx(gpp, rule = 2, na.rm = FALSE)) %>% #rule 2 means edges are assigned last value
  ungroup()

#make sure sites are ordered alphabetically
pggpp <- pggpp %>%
  arrange(site) %>%
  mutate(site = factor(site, ordered = TRUE))

#extract alpha part of code
pggpp <- pggpp %>%
  mutate(farm = substr(site, 1, 4),
         sitenum = as.integer(substr(site, 5, 5)))
```


## Automatic Time Series Decompositions
Using interpolated values for all decompositions - so that it will not error.

In below:
+ the season window specifies how fast the seasonal component of the decomposition can vary
+ the trend window specifies how fast the trend can vary

```{r stl_decomposition_additive}
decomposition <- pggpp %>%
  filter(site %in% c("ARCH1", "BELL1")) %>%
  STL(lininterp_gpp ~ trend(5*8, degree = 1) + season("3 year", degree = 0))
decomposition %>% 
  autoplot()
```

Remainders are quite autocorrelated even with a trend term. Multiplicate might do better.


Multiplicative decompisition with STL
```{r stl_decomposition_additive}
decomposition <- pggpp %>%
  filter(site %in% c("ARCH1", "BELL1")) %>%
  STL(log(lininterp_gpp + 1) ~ trend(6*8, degree = 1) + season("1 year", degree = 1))
decomposition %>% 
  autoplot()
```
The remainders are about a 3rd of the season variation.

Something in between multiplicative and additive:
```{r stl_decomposition_additive}
decomposition <- pggpp %>%
  filter(site %in% levels(site)[c(1, 5, 9)]) %>%
  STL(box_cox(lininterp_gpp + 1, lambda = 0.5) ~ trend(5*8, degree = 1) + season("1 year", degree = 0))
decomposition %>% 
  autoplot()
```

Multiplicative decompisition with STL with outliers capped (really it would be better to delete but the code doesn't allow that)
```{r stl_decomposition_additive}
edges <- quantile(pggpp$gpp, probs = c(0.01, 0.99), na.rm = TRUE, names = TRUE)
decomposition <- pggpp %>%
  filter(site %in% c("ARCH1", "BELL1")) %>%
  mutate(lininterp_gpp = case_when(lininterp_gpp < edges[[1]] ~ edges[[1]],
                                   lininterp_gpp > edges[[2]] ~ edges[[2]],
                                   TRUE ~ lininterp_gpp)) %>%
  STL(log(lininterp_gpp +1) ~ trend(6*8, degree = 1) + season("1 year", degree = 1))
decomposition %>% 
  autoplot()
```

It is difficult to interpret much from this figure, nothing appears different to when outliers are not capped.

### Classical decomposition
Uses moving averages
```{r classicaldecomp}
pggpp %>%
  filter(sitenum == 1) %>% #keep only first site at each farm
  filter(farm %in% unique(farm)[1:10]) %>%
  classical_decomposition(lininterp_gpp ~ season("1 year"), type = "multiplicative") %>%
  autoplot()
```

The seasonal variation and the trend variation are both smaller than the remainder! 
Furthermore the remainders appear to be autocorellated - suggestions decomposition is missing systematic parts.
Interesting that the trend says roughly constant for a year over spring, for many of the years.

##  Correlation Between Precipitation Derivatives
It seems like precipitation has a maximum impact *once a place is very wet, more rain isn't going to make it grow faster*. This means that the effect of precipitation in the last month depends on the precipitation over the last 6 months (i.e. there is interaction). The effect will also interact with the GPP of that site of that particular day in past years.

 + Use cumulative-monthly precipitation (backwards in time?)
 + Use monthly precipitation with interactions between months?
 + Use seasonal residuals of precipitation?
 + Use interactions between cumulative precipitation and season, and past GPP?
 
*The goal:* detect differences in how a farms GPP responds to  rainfall. Most likely the observed GPP will spike and then as farmers add more animals it will return to normal levels. We expect over-grazing will lead to smaller/non-existent spikes.

*Proposal:* high rate of change in GPP will correspond to spikes. For gpp sufficiently high, grazing is increased to maximise profits and the GPP increase is slower. This suggests a model for the rate of GPP change as a function of current/previous GPP, day of the year, and recent rainfall, with interactions between all of these.

+ high GPP ==> low rate of GPP increase regardless of other covariates
+ high rainfall and low GPP ==> high rate of GPP increase
+ moderate rainfall, spring and low GPP ==> moderate rate of GPP increase


### Correlation within rainfall time-series.
#### Cumulative rainfall.
```{r calcumulativerainfall}
pggpp_cumsum <- pggpp %>% 
  group_by_key() %>%
  mutate(pg_cumsum = cumsum(pg)) %>%
  mutate(pg_1to8 = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 8), # 1 to 8 days behind
         pg_1to16 = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 16), # 1 to 16 days behind
         pg_1to1m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 31),
         pg_1to2m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 2*31),
         pg_1to3m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 3*31),
         pg_1to4m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 4*31),
         pg_1to5m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 5*31),
         pg_1to6m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 6*31),
         pg_1to7m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 7*31),
         pg_1to8m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 8*31),
         )

pggpp_cumsum %>%
  filter(site == "BELL1") %>%
  filter_index("2010") %>%
  pivot_longer(cols = c("pg_1to1m",
                        "pg_1to2m",
                        "pg_1to3m",
                        "pg_1to4m",
                        "pg_1to5m",
                        "pg_1to6m",
                        "pg_1to7m",
                        "pg_1to8m")) %>%
  ggplot() +
  geom_line(aes(x = times, y = value, col = name))
```

In these cumulative amounts you can see that large increases in PG are included in all cumulative amounts (as they all start at 1 day), and that this is not the case for a lack of precipitation.

Should I be looking for linear relationships? Or monotonic relationships? (which correlations to test for?) For a linear model Pearson's Correlation is appropriate I think, but for GAM modelling I think Kendall or Spearman would do better (since they indicate monotonic relationships).
```{r cumulativecorrelations}
cormat <- pggpp_cumsum %>%
  as_tibble() %>%
  #group_by(site) %>%
  select(c("pg", grep("pg_1", names(pggpp_cumsum), value = TRUE))) %>%
  group_map(.f = ~ cor(.x, use = "pairwise.complete.obs"))
reshape2::melt(cormat[[1]], value.name = "Pearson Correlation") %>%
  ggplot() +
  geom_tile(aes(x = Var1, y = Var2, fill = `Pearson Correlation`)) +
  geom_text(aes(x = Var1, y = Var2, label = format(`Pearson Correlation`, digits = 1))) +
  scale_fill_viridis_c() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  ggtitle("Pearson Correlation between cumulative precipitation averaged across all sites")
```

The correlations between first 8, 16, 1 month, 2 month and 3 month are not prohibitively high (up to 0.88) I think. But correlatations between longer cumulative rainfalls (3m, 4m. 5m, ...) is getting really high. 
Suppose we wanted to fit only using predictors with correlation less than 0.9. Could choose:
pg, 8, 16, 1m, 2m, 3m, 5m, 8m.

Is this overall correlation matched within groups?
```{r cumulativecorrelations_grouped}
cors <- pggpp_cumsum %>%
  as_tibble() %>%
  group_by(site) %>%
  select(c("pg", grep("pg_1", names(pggpp_cumsum), value = TRUE))) %>%
  group_map(.f = ~ cor(.x, use = "pairwise.complete.obs"))
cors <- c(cors, cormat)
names(cors) <- c(as.character(group_keys(pggpp_cumsum)$site), "all")
corsmelt <- lapply(cors, function(x) reshape2::melt(x - cormat[[1]], value.name = "Diff Pearson Correlation"))
corstbl <- bind_rows(corsmelt, .id = "site")

corstbl %>%
    mutate(farm = substr(site, 1, 4),
         sitenum = as.integer(substr(site, 5, 5))) %>%
  filter(site %in% c(paste0(unique(farm), "1"), "all")) %>%
  ggplot() +
  facet_wrap(~ site) +
  geom_tile(aes(x = Var1, y = Var2, fill = `Diff Pearson Correlation`)) +
  scale_fill_viridis_c(limits = extendrange(r = range(corstbl$`Diff Pearson Correlation`), 0.1)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  coord_fixed() +
  theme(strip.text.x = element_text(size = 8)) +
  ggtitle("Pearson Correlation between cumulative precipitation for each farm")
```
 
 The range of difference in correlation computer per-site or across sites is `r range(corstbl$`Diff Pearson Correlation`)`. It is small enough that I think the correlation assessment of all sites together is enough, and that the suggested less-correlated variables of pg, 8, 16, 1m, 2m, 3m, 5m, 8m could work well.

#### Spearman Rank Correlation for Investigating Monotonic Function Relationships
```{r cumulativecorrelations_spearman}
cormat <- pggpp_cumsum %>%
  as_tibble() %>%
  #group_by(site) %>%
  select(c("pg", grep("pg_1", names(pggpp_cumsum), value = TRUE))) %>%
  group_map(.f = ~ cor(.x, use = "pairwise.complete.obs", method = "spearman"))
reshape2::melt(cormat[[1]], value.name = "Spearman Correlation") %>%
  ggplot() +
  geom_tile(aes(x = Var1, y = Var2, fill = `Spearman Correlation`)) +
  geom_text(aes(x = Var1, y = Var2, label = format(`Spearman Correlation`, digits = 1))) +
  scale_fill_viridis_c() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  ggtitle("Spearman Correlation between cumulative precipitation averaged across all sites")
```

The above Spearman correlation has a similar message to the Pearson's correlation earlier. Thus choosing pg, 8, 16, 1m, 2m, 3m, 5m, 8m continues to be a good option.

```{r cumulativecorrelations_grouped_spearman}
cors <- pggpp_cumsum %>%
  as_tibble() %>%
  group_by(site) %>%
  select(c("pg", grep("pg_1", names(pggpp_cumsum), value = TRUE))) %>%
  group_map(.f = ~ cor(.x, use = "pairwise.complete.obs", method = "spearman"))
cors <- c(cors, cormat)
names(cors) <- c(as.character(group_keys(pggpp_cumsum)$site), "all")
corsmelt <- lapply(cors, function(x) reshape2::melt(x - cormat[[1]], value.name = "Diff Spearman Correlation"))
corstbl <- bind_rows(corsmelt, .id = "site")

corstbl %>%
    mutate(farm = substr(site, 1, 4),
         sitenum = as.integer(substr(site, 5, 5))) %>%
  filter(site %in% c(paste0(unique(farm), "1"), "all")) %>%
  ggplot() +
  facet_wrap(~ site) +
  geom_tile(aes(x = Var1, y = Var2, fill = `Diff Spearman Correlation`)) +
  scale_fill_viridis_c(limits = extendrange(r = range(corstbl$`Diff Spearman Correlation`), 0.1)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  coord_fixed() +
  theme(strip.text.x = element_text(size = 8)) +
  ggtitle("Spearman Correlation between cumulative precipitation for each farm")
```

Again the difference between the Spearman correlation of each site, and the Spearman correlation ignoring sites is small, with a range of `r range(corstbl$`Diff Spearman Correlation`)`. Thus interpretation of the all-in Spearman correlations is sufficient.

#### Seasonal Autocorrelation of GPP
```{r BELL1_acf_1yday}
pggpp_yr <- pggpp %>%
  mutate(year = year(times)) %>%
  update_tsibble(key = c(yday, site), index = year)

pggpp_yr %>%
  filter(site == "BELL1") %>%
  filter(yday == 1) %>%
  ACF(gpp) %>% autoplot() +
  ggtitle("Autocorrelation of 1st day of the year GPP at BELL1")
```

The usual dashed lines in a correlogram are 95% significance levels for *white noise* processes. We would expect 95% of observed sample autocorrelations to be within the two lines:
"For a white noise series, we expect 95% of the spikes in the ACF to lie within \(\pm 2/\sqrt{T}\) where \(T\) is the length of the time series. It is common to plot these bounds on a graph of the ACF (the blue dashed lines above). If one or more large spikes are outside these bounds, or if substantially more than 5% of spikes are outside these bounds, then the series is probably not white noise." [Hyndman, Rob J, and George Athanasopoulos. Forecasting: Principles and Practice. Melbourne, Australia: Otexts, Accessed Jan 17, 2020. Otexts.com/fpp3/; \S2.9]

They are very large here because the time series is very short (19 years).

```{r BELL1_seasonalacfs}
suppressWarnings(acfs_bell1 <- pggpp_yr %>%
  filter(site == "BELL1") %>%
  filter(yday %in% seq(1, 366, by = 8)) %>%
  ACF(gpp))
acfs_bell1 %>%
  ggplot() +
  geom_segment(aes(x = lag + yday/500, xend = lag + yday/500, y = 0, yend = acf, col = yday)) +
  geom_hline(yintercept = c(-1, 1) *2 / sqrt(nrow(pggpp_yr %>% filter(site == "BELL1", yday ==1)))  ,
             col = "blue",
             lty = "dashed") +
  scale_color_viridis() +
  ggtitle("Sample Seasonal Autocorrelation for BELL1's GPP, by day of the year")

acfs_bell1 %>%
  summarise(meanacf = mean(acf)) %>%
  ggplot() +
  geom_segment(aes(x = lag, xend = lag, y = 0, yend = meanacf)) +
  ggtitle("Mean Sample Seasonal Autocorrelation for BELL1's GPP")
```

I'm not sure what lines, if any, to draw in the above. It will depend on what the actual distribution of sample autocorrelation is for white noise.

```{r seasonalacfs_allsites}
pggpp_yr %>%
  filter(yday %in% seq(1, 366, by = 8)) %>%
  anyNA()
# it looks like all sites are on the same 8 day schedule! Some preprocessing perhaps?

suppressWarnings(acfs <- pggpp_yr %>%
  filter(yday %in% seq(1, 366, by = 8)) %>%
  ACF(gpp))

acfs %>%
  summarise(meanacf = mean(acf)) %>%
  ggplot() +
  geom_segment(aes(x = lag, xend = lag, y = 0, yend = meanacf)) +
  ggtitle("Mean Sample Seasonal Autocorrelation for all sites GPP")
```

Interesting that on average there is anti-correlation. Perhaps there is a trend?

```{r seasonalacfs_more} 
acfs %>%
  group_by(farm = substr(site, 1, 4)) %>%
  summarise(meanacf = mean(acf)) %>%
  ggplot() +
  facet_wrap(~farm) +
  geom_segment(aes(x = lag, xend = lag, y = 0, yend = meanacf)) +
  ggtitle("Mean Sample Seasonal Autocorrelation for all sites GPP")
```

Also, in a very real sense the 19 years is not enough to detect autocorrelation - it may as well be white noise. Is 19 years enough to fit a mean GPP for each day of the year? The standard deviation would be the s.d. of GPP divided by sqrt(19), so without any assessment between days the mean could have (1/4) the standard deviation of the observed GPP.

How about the partial ACF (PACF)
```{r partialautocorr}
suppressWarnings(pacfs <- pggpp_yr %>%
  filter(yday %in% seq(1, 366, by = 8)) %>%
  PACF(gpp))

pacfs %>%
  group_by(farm = substr(site, 1, 4)) %>%
  summarise(meanpacf = mean(pacf)) %>%
  ggplot() +
  facet_wrap(~farm) +
  geom_segment(aes(x = lag, xend = lag, y = 0, yend = meanpacf)) +
  ggtitle("Mean Sample Seasonal Partial-Autocorrelation for each farm")


pacfs %>%
  filter(site == "BELL1") %>%
  ggplot() +
  geom_segment(aes(x = lag + yday/500, xend = lag + yday/500, y = 0, yend = pacf, col = yday)) +
  scale_color_viridis() +
  ggtitle("Sample Seasonal Partial - Autocorrelation for BELL1's GPP, by day of the year")
```

Partial auto-correlation does not reduce with lag, which is consistent with there being no detectable autocorrelation.

__there should be a way to pool these estimates of autocorrelation to say something stronger__
If each site and day is considered white noise with a mean and standard deviation particular to the site then: no greater power as each site+day must be inferred independently.
If neighbouring days are considered to have smilar properties, for example the mean and standard deviation changes smoothly with the day of the year, then the mean and standard deviation could be fitted and improve on the 1/4 reduction in variation in of the estimate of mean GPP...
*But maybe better to avoid without doing anything seasonal for GPP as it doesn't appear needed*.

### Add seasonal precipitation residuals
First attempt is to use STL decomposition (which is smoothing), which can't actually *predict* seasonal precipitation so could be bad use!
```{r seasonalprecipitation}
decomposepg <- pggpp %>%
  group_by_key() %>%
  STL(pg ~ season(period = "1 year", window = "periodic"), iterations = 3)
decomposepg <- decomposepg %>%
  inner_join(pggpp, by = NULL)
decomposepg %>%
  filter(sitenum == 1) %>% #keep only first site at each farm
  filter(farm %in% unique(farm)[c(1, 5, 10)]) %>%
  filter_index("2011") %>%
  ggplot() +
  facet_wrap(~ site) +
  geom_line(aes(x = times, y = trend + `season_1 year`), col = "blue") +
  geom_point(aes(x = times, y = pg), size = 0.1) 
```

Lets try an actual model! Zero-inflated distribution: Tweedie with 1 < p < 2.
First lets get experience looking at the histograms of the pg data and tweedie distributions.
```{r tweedieeg}
library(tweedie)
invisible(tweedie.plot(seq(0, 30, by = 0.1), mu = 0.2, phi = 0.5, power = 1.2))
```
```{r tweedieforpg}
prop0 <- mean(pggpp$pg == 0)
pggpp %>% 
  ggplot() +
  geom_histogram(aes(x = pg, y = stat(density) * (1 - prop0)), data = function(x) x %>% filter(pg > 0), bins = 30, col = "grey") +
  geom_density(aes(x = pg, y = stat(density) * (1 - prop0)), data = function(x) x %>% filter(pg > 0)) +
  geom_point(aes(x = 0, y = prop0)) +
  xlim(0, 40)
```


```{r fitgamwithtweedie_BELL1}
library(mgcv)
pgmodel <- gam(pg ~ s(yday, bs = "cc", k = 20), family = tw(),
             data = pggpp %>% filter(site == "BELL1"),
             knots = list(yday = c(0, 10)))
plot(pgmodel)
gam.check(pgmodel)
acf(pgmodel$residuals)
```

The QQ plot suggests that the distribution of observed residuals matches the assumption of a Tweedie distribution (with fitted p).
The ACF plot suggests some autocorrelation of level 1.

Given that the distribution of residuals is a non-symmetric distribution (Tweedie == gamma-poisson compound), then I wonder if using them makes sense for predicting gpp.

Trying using autocorrelation and the `p` found above.
```{r fitgamwithtweedie_BELL1}
library(mgcv)
pgmodel_AR1 <- gamm(pg ~ s(yday, bs = "cc", k = 20), family = Tweedie(p = 1.781, link = "log"),
             data = pggpp %>% filter(site == "BELL1"),
             correlation = corAR1(form = ~1),
             knots = list(yday = c(0, 10)))
plot(pgmodel_AR1$gam)
plot(pgmodel_AR1$lme)
acf(pgmodel_AR1$lme$residuals[, 1])
gam.check(pgmodel_AR1$gam)
acf(pgmodel_AR1$gam$residuals)
```

Fitting took a LONG time, and this just for a single site.
I expected the correlations of the residuals to drop, but they didn't.
Also I do not know what the piecewise-linear nature of the QQ plot means.

I've concluded that the seasonal-residual daily precipitation will not be very useful as the distribution around the seasonal precipitation is highly unsymmetric. The mean daily precipitation as fitted in the above models might be useful though.
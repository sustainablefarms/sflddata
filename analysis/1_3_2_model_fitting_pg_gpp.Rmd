---
title: "Investigating Log-Linear Model Fit"
author: "Kassel Hingee"
date: "10/01/2020"
output: 
  html_document: 
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, root.dir = "..")
out <- lapply(c("sf", "tsibble", 'lubridate', "viridis",
                'ggplot2', 'tidyr', 'grid', 'gridExtra', 
                'feasts', 'dplyr', 'gtable', 'fable'),
       library, character.only = TRUE)
out <- lapply(paste0("../functions/", list.files("../functions/")), source)
```

```{r preparedata, echo = FALSE}
#load data and convert to tsibbles
load("../private/data/remote_sensed/pg_daily.Rdata")
pg_daily$times <- as_date(pg_daily$times)
pg <- pg_daily %>%
  pivot_longer(-times, names_to = "site", values_to = "pg") %>%
  as_tsibble(key = site, index = times)
load("../private/data/remote_sensed/gpp_8d.Rdata")
gpp <- gpp_8d %>%
  pivot_longer(-times, names_to = "site", values_to = "gpp") %>%
  as_tsibble(key = site, index = times)
pggpp <- as_tsibble(dplyr::full_join(pg, gpp, by = c("times", "site")), key = site, index = times)
pggpp <- pggpp %>%
  mutate(yday = yday(times)) %>%
  group_by_key() %>% #key is site
  mutate(lininterp_gpp = zoo::na.approx(gpp, rule = 1, na.rm = FALSE))

pggpp_train <- pggpp %>%
  filter_index(. ~ "2016-12-31")
pggpp_test <- pggpp %>%
  filter_index("2017-01-01" ~ .)


pggpp_train_concatsites <- pggpp_train %>%
 mutate(siteindx = as.numeric(factor(site, levels = unique(pggpp$site)))) %>%
 mutate(alttimes = times - days(366 * 30 * siteindx)) %>% #can't use year periods due to leap years
 ungroup() %>%
 update_tsibble(key = NULL, index = "alttimes") %>%
 fill_gaps()
```

```{r loadfittedmodel}
fit5 <- readRDS("../analysis/fit_fmla5_concatsites.rds")
```

The model investigated here is a linear model of log(GPP + 0.001) with predictors:
```{r modelfmla, echo = FALSE}
tidy(fit5)$term
```

Note that that 0.001 is designed to avoid taking the logarithm of 0 (about 0.5% of all GPP measurements are 0). The value was chosen to be a tiny fraction of the range of observed GPP, which was from 0 to about 15.

The first terms are linearly interpolated log(GPP +  0.001) on the same day of the year for 1, 2, 3, 4, and 5 years in the past.
The intercept term is the usual to-be-fitted constant offset independent of predictors.
The other terms are the sum of precipitation across 7 day windows, lagged by 0, 7, 14, ..., 63 days (0 weeks through to 9 weeks).

In mathematical terminology the model is:
\[
\begin{aligned}
 \log(Y_i(t) + 0.001) & = & \theta_{10} + \theta_{11} \log(Y_i(t - 1yr) + 0.001) +  \theta_{12} \log(Y_i(t - 2yr) + 0.001) + \theta_{13}\log(Y_i(t - 3yr) + 0.001) +\\
&& \theta_{14}\log(Y_i(t - 4yr) + 0.001) + \theta_{15}\log(Y_i(t - 5yr) + 0.001)  +\\
&& \theta_{20} X_i(t) +  \theta_{22}X_i(t - 1w) + ... + \theta_{29}X_i(t - 9w) +
\epsilon_i(t)\\
\iff && \\
Y_i(t) + 0.001 & = & \exp(\theta_{10}) 
  (Y_i(t - 1yr) + 0.001)^{\theta_{11}}
  (Y_i(t - 2yr) + 0.001)^{\theta_{12}}
  (Y_i(t - 3yr) + 0.001)^{\theta_{13}}\\
&&  (Y_i(t - 4yr) + 0.001)^{\theta_{14}}
  (Y_i(t - 5yr) + 0.001)^{\theta_{15}} \\
&& \exp(X_i(t))^{\theta_{20}} 
  \exp(X_i(t - 1w))^{\theta_{21}}...
  \exp(X_i(t - 9w))^{\theta_{29}}
  \exp(\epsilon_i(t))\\
\end{aligned}
\]
where 

+ $Y_i$ is the gpp at site $i$
+ $X_i$ is the sum of rain in the last 7 days at site $i$
+ $t$ is the date
+ $\epsilon_i(t)$ is a collection of i.i.d random variables, each with Gaussian distribution with mean of $0$ and standard deviation independent of the site $i$.

This model can be viewed as a familiar log-linear model (the error term is assumed to have no autocorrelation). It was fitted using the `fable` function `ARIMA` which uses base R's `arima` function. However, I am fairly sure that fitting using `stats::lm()` would have worked also, but involved further coding.

The terms are based on the assumption that GPP is seasonal (i.e. can be predicted from the GPP on the same day in previous years) and that it is dependent on precipitation in the last 9 weeks.


## Fitted Parameters
The fitted parameters for this model are
```{r fittedparams, echo = FALSE}
print(tidy(fit5) %>% select(- .model), n = 17)

fit5 %>%
  tidy() %>%
  mutate(term = factor(term, levels = tidy(fit5)$term, ordered = TRUE)) %>%  #makes sure plot maintains order of terms
  filter(term != "Intercept") %>%
  ggplot() +
  #geom_bar(aes(x = term, y = estimate), stat = "identity") +
  geom_pointrange(aes(x = term, ymin = estimate - 2 * std.error, ymax = estimate + 2 * std.error, y = estimate),
                  stat = "identity", fatten = 0.1, na.rm = TRUE) +
  ylim(c(-0.1, 0.5)) +
  coord_flip()
```

Intercept is not shown here for convenience as the estimated intercept is much larger than the other coefficients.


*If* the model assumptions are correct then all the predictors are statistically significant (the above plot has the standard error range represented by segments).

## Residual analysis

```{r residuals_training, echo = FALSE}
est_resid_training <- inner_join(residuals(fit5), fitted(fit5))
stopifnot(all.equal(est_resid_training$alttimes, pggpp_train_concatsites$alttimes)) #checking that regenerated 
est_resid_training <- right_join(est_resid_training, pggpp_train_concatsites) %>%
                filter(!is.na(times)) %>%
                update_tsibble(key = site, index = times)


est_resid_training %>%
  ggplot() +
  geom_histogram(aes(x = .resid), na.rm = TRUE, bins = 80) +
  ggtitle("Histogram of Residuals: all days and sites combined")

est_resid_training %>%
  ggplot() +
  geom_abline(aes(intercept = 0, slope = 1), col = "grey", lwd = 0.5) +
  geom_qq(aes(sample = .resid), na.rm = TRUE)
  ggtitle("Q-Q plot of residuals: all days and sites combined")

est_resid_training %>%
  mutate(lagloggpp = lag(log(lininterp_gpp + 0.001)), n = 365.25) %>%
  ggplot() +
  geom_bin2d(aes(x = lagloggpp, y = .resid), bins = 60) +
  scale_fill_viridis(trans = "log") +
  ggtitle("Density of residual occurence against GPP of one year earlier")
```

The histogram reveals a cluster of highly negative residuals.
The Q-Q plot is highly non-linear, confirming that residuals are not normally distributed.
From the plot of density of residual occurences against lagged log GPP:

a. the highly negative residuals occur only when the previous year has nearly 0 GPP (0.28 or smaller).
b. the highly negative residuals appear to be separate from the residuals (there are very few residuals bettween - 5 and -2.5, and these do not occur at the same lag log gpp).
c. the remaining residuals have a mean that depends on the lagged log GPP values; it is difficult to tell if the the variance of the residuals also depends on the value of lagged log GPP.

When the previous year has positive GPP, and the current year has 0 GPP. The *only* multiplicative models that can predict this involve multiplying by something close to $0 = \exp(-\infty)$. Thus we should expect any multiplicative models to have large residuals for these situations, noting that the residual is only large in *transformed* units, the residuals in natural GPP units is less than 0.5.

1. try adding larger numbers than 0.001 to reduce the emphasis (in terms of model fitting objective) on close-to-zero values of GPP. This feel's like a poor option though as it brings lots of questions about why the model has that form. It might be better to instead assume that the error terms $\epsilon_i$ has an extreme-value component and then use extreme-value methods (somehow).
2. OR exclude very low GPP observations from the training data, and have a caveat that very low GPP values are not modelled well.


## Plots of forecasted values

### For the training data
```{r plotforecasted_training, echo = FALSE}
est_resid_training %>%
  filter(site %in% unique(pggpp$site)[c(1, 5, 9, 19, 54)]) %>%
  filter_index("2005" ~ .) %>%
  pivot_longer(c(.fitted, gpp), names_to = "valname") %>%
  as_tsibble(key = c(site,valname), index = times) %>%
  mutate(value = zoo::na.approx(value, rule = 1, na.rm = FALSE)) %>%
  ggplot() +
  facet_grid(rows = vars(site)) +
  geom_line(aes(x = times, y = value, col = valname, lty = valname), na.rm = TRUE) +
  ggtitle("Forecasts and Observed GPP: training data")
```
In the above it appears that the fitted values have been automatically transformed back to GPP (technically the model fitted log(GPP + 0.001)). 
Note the histogram of fitted values is similar to the histogram of observed GPP, and that the inverse transorfmation, $\exp(x) - 0.001$, of the fitted values is completely different to the historgram of observed GPP values.


The 2007 spring peak is significantly underestimated. Is this because GPP of previous year was so low and precipitation doesn't have a big impact in the model?

The first GPP peak in 2010 at GEDD2 is not forecasted (it also wasnt forecasted in the additive model). The rain causing this peak was only a few weeks earlier, suggesting that it something about the way precipitation is incorporated into the model.

1. correlation analysis, a variety of transforms and derivative precipitation infomration, would be useful.

Like in the additive model, the forecasted GPP values for 2006 do not capture the drop in GPP values earlier than October.

Some peaks are forecasted to be much higher than they were. How is the 2016 spring peak forecast really high for GEDD2 whilst accurate for CHAL3?


### The peak of 2016
```{r gedd2016, echo = FALSE}
pg_by_yearmonth <- pggpp %>%
  group_by_key() %>%
  index_by(ym = yearmonth(times)) %>%
  summarise(monthlypg = sum(pg))
mean_monthly_pg <- pg_by_yearmonth %>% 
  as_tibble() %>%
  group_by(site, m = month(ym)) %>%
  summarise(mean_monthly_pg = mean(monthlypg))

est_resid_training %>%
  mutate(m = month(times)) %>%
  left_join(mean_monthly_pg, by = c("site", "m")) %>%
  filter(site %in% c("CHAL3", "GEDD2")) %>%
  filter_index("2015" ~ "2016") %>%
  pivot_longer(c(pg, .fitted, gpp), names_to = "valname") %>%
  as_tsibble(key = c(site,valname), index = times) %>%
  mutate(value = zoo::na.approx(value, rule = 1, na.rm = FALSE)) %>%
  ggplot() +
  facet_grid(rows = vars(valname), cols = vars(site), scales = "free_y") +
  geom_line(aes(x = times, y = value), na.rm = TRUE) +
  ggtitle("Forecasts and Observed GPP: training data") +
  geom_line(aes(x = times, y = mean_monthly_pg),
            function(x) dplyr::filter(x, valname == "pg"), #so only plots for pg facet
            lty = "dotted") + 
  stat_summary_bin(aes(times, value),
         data = function(x) dplyr::filter(x, valname == "pg"), #so binning only applies to pg data
         colour = "blue",
         fun.y = sum,
         binwidth = 30,
         geom = "step")
```


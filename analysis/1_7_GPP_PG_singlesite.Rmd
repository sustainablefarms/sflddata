---
title: "LIND1 GPP vs PG fit"
author: "Kassel Hingee"
date: "03/04/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
out <- lapply(c("sf", "tsibble", 'lubridate', "viridis",
                'ggplot2', 'tidyr', 'grid', 'gridExtra', 
                'feasts', 'dplyr', 'gtable', 'fable',
                'mgcv'),
       library, character.only = TRUE)
out <- lapply(paste0("../functions/", list.files("../functions/")), source)
```

```{r preparedata}
#load data and convert to tsibbles
load("../private/data/remote_sensed/pg_daily.Rdata")
pg_daily$times <- as_date(pg_daily$times)
pg <- pg_daily %>%
  pivot_longer(-times, names_to = "site", values_to = "pg") %>%
  as_tsibble(key = site, index = times)
load("../private/data/remote_sensed/gpp_8d.Rdata")
gpp <- gpp_8d %>%
  pivot_longer(-times, names_to = "site", values_to = "gpp") %>%
  as_tsibble(key = site, index = times)
pggpp <- as_tsibble(dplyr::full_join(pg, gpp, by = c("times", "site")), key = site, index = times)

# add times breakdowns
pggpp <- pggpp %>%
  mutate(yday = yday(times),
         year = year(times))
  

#interpolate gpp
pggpp <- pggpp %>%
  group_by_key() %>% #key is site
  mutate(lininterp_gpp = zoo::na.approx(gpp, rule = 2, na.rm = FALSE)) %>% #rule 2 means edges are assigned last value
  ungroup()

#make sure sites are ordered alphabetically
pggpp <- pggpp %>%
  arrange(site) %>%
  mutate(site = factor(site, ordered = TRUE))

#separate alpha part of site code
pggpp <- pggpp %>%
  mutate(farm = factor(substr(site, 1, 4)),
         sitenum = factor(as.integer(substr(site, 5, 5))))


# Add cumulative rainfalls
pggpp <- pggpp %>% 
  group_by_key() %>%
  mutate(pg_cumsum = cumsum(pg)) %>%
  mutate(pg_1to7 = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 8), # 1 up to 8 days behind (excluding 8th day)
         pg_1to15 = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 16), # 1 to 16 days behind
         pg_1to1m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 31),
         pg_1to2m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 2*31),
         pg_1to3m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 3*31),
         pg_1to4m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 4*31),
         pg_1to5m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 5*31),
         pg_1to6m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 6*31),
         pg_1to7m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 7*31),
         pg_1to8m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 8*31),
         pg_1to10m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 10*31),
         pg_1to12m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 12*31),
         pg_1to14m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 14*31),
         ) %>%
  mutate(pg_8d = (pg_cumsum - lag(pg_cumsum, n = 8))/8.0,
         #0 to day 7 (8 days) cumulative rainfall to correspond with gaps in GPP
         pg_24d = (pg_cumsum - lag(pg_cumsum, n = 3*8)) / 24) %>% 
         # every 24 days (3*8) of GPP should be less correlated (given average GPP), this pg_24d corresponds to the rain through that period
  ungroup()
```

```{r seasonalreferences}
# simple median of values
ydaymedian <- pggpp %>%
  filter(yday %in% seq(1, 366, by = 8)) %>%
  group_by(site) %>%
  index_by(yday) %>%
  summarise(gpp.ydaymed = median(gpp),
            pg_8d.ydaymed = median(pg_8d),
            pg_24d.ydaymed = median(pg_24d),
            pg_1to5m.ydaymed = median(pg_1to5m, na.rm = TRUE))
pggpp <- left_join(pggpp, ydaymedian, by = c("site", "yday"))
```

```{r preparetraintest}
train <- pggpp %>%
  filter(yday %in% seq(1, 366, by = 8)) %>%
  filter(sitenum == 1) %>% #so not doubling up at farms
  filter_index(. ~ "2016-12-31") %>%
  dplyr::select(-pg_cumsum, -lininterp_gpp) %>%
  mutate(gpp = if_else(gpp < 0.1, as.double(NA), gpp)) #remove outlying GPP values that are super low

test <- pggpp %>%
  filter_index("2017-01-01" ~ .) %>%
  filter(sitenum == 1) %>% #so not doubling up at farms
  filter(yday %in% seq(1, 366, by = 8)) %>%
  dplyr::select(-pg_cumsum)
```


In the above very small GPP values have been removed. There was `r sum(is.na(train$gpp))` of them, which corresponds to `r sum(is.na(train$gpp)) / length(train$gpp)` of the data.

## Site LIND1
```{r select_pggpp_for_lind1}
train <- train %>%
  filter(site == "LIND1")
test <- test %>%
  filter(site == "LIND1")
```

```{r view_lind1}
train %>%
  mutate(st_gpp = gpp / gpp.ydaymed) %>%
  pivot_longer(cols = c("st_gpp", "pg_8d", "pg_24d")) %>%
  ggplot() +
  geom_hline(aes(yintercept = 1), col = "grey", lty = "dashed") +
  geom_line(aes(x = yday, y = value, col = name)) +
  facet_wrap(vars(year)) +
  ylim(c(0, 10))
```


### Attempt a garch model
`garch()` from `tseries` package does not appear to allow external predictors.
Similarly the `fGarch` package does not have any ability for external regressors.
However the `rugarch` package does.

```{r garch_m1}
library(rugarch)

filtertrain5 <- function(x){
  x <- x %>% filter(!is.na(pg_1to14m),
                    yday %in% seq(1, 366, by = 1 * 8), #reduce correlation in data fitting and reduce fitting time
                    !is.na(gpp))
  return(x)
}

Xproto <- model.frame(~ pg_1to5m * I(1 / pg_1to5m.ydaymed) * pg_24d,
             data = train %>% filtertrain5())
Xproto <- scale(Xproto)
Xscales <- attributes(Xproto)

X <- model.matrix(~ 
              pg_1to5m * I.1.pg_1to5m.ydaymed. * pg_24d,
             data = data.frame(Xproto))[, -1]  #doesn't fit when intercept is included!

Y <- train %>%
  filtertrain5() %>%
  mutate(y = box_cox(gpp / gpp.ydaymed, lambda = 0.1414141)) %>%
  as_tibble() %>%
  select(y) %>%
  as.matrix() %>%
  as.vector()

acfs <- acf(Y)
pacfs <- pacf(Y)

garch_m1_spec <- ugarchspec(variance.model = list(
  model = "sGARCH", garchOrder = c(1, 1)),
  mean.model = list(armaOrder = c(1, 1),
                    external.regressors = X),
  start.pars = list(mu = median(Y),
                    ar1 = pacfs$acf[1, 1, 1],
                    ma1 = acfs$acf[1, 1, 1],
                    alpha1 = 0.8,
                    beta1 = 0.8) )
 

garch_m1 <- ugarchfit(garch_m1_spec, Y,
                      # out.sample = 500,
                      solver = "hybrid",
                      solver.control = list(parallel = TRUE),
                      fit.control = list(stationarity = TRUE, scale = 1))

# print(garch_m1)
plot(garch_m1, which = "all")
infocriteria(garch_m1)
```

```{r fit_withoutstartparameters}
garch_m1_spec <- ugarchspec(variance.model = list(
  model = "sGARCH", garchOrder = c(1, 1)),
  mean.model = list(armaOrder = c(1, 1),
                    external.regressors = X))
 

garch_m1_autostart <- ugarchfit(garch_m1_spec, Y,
                      # out.sample = 500,
                      solver = "hybrid",
                      solver.control = list(parallel = TRUE),
                      fit.control = list(stationarity = TRUE, scale = 1))

# print(garch_m1_autostart)
plot(garch_m1_autostart, which = "all")
tibble(times = zoo::index(fitted(garch_m1_autostart)), fit = as.numeric(fitted(garch_m1_autostart)), actual = Y) %>%
  pivot_longer(cols = c("fit", "actual")) %>%
  ggplot() +
  geom_line(aes(x = times, y = value, col = name, lty = name))
infocriteria(garch_m1_autostart)
show(garch_m1_autostart)
```

The fitted model using automatically determined initial search values satisfies assumptions better, particularly the normal distribution of the residuals. It also has much better information criteria values. I will investigate its predictions in the following.

```{r garch_m1_autostart_prediction}
# fcst_10ahead_uncertainty <- ugarchboot(garch_m1_autostart,
#                            method = "partial", #uncertainy is only in model's randmoness, uncertainty of fitted parameters not included.
#                            n.ahead = 10)

Xtestproto <- model.frame(~ pg_1to5m * I(1 / pg_1to5m.ydaymed) * pg_24d,
             data = test %>% filtertrain5())
Xtestproto <- scale(Xtestproto,
                    center = attr(Xproto, "scaled:center"),
                    scale = attr(Xproto, "scaled:scale"))
Xtest <- model.matrix(~ 
              pg_1to5m * I.1.pg_1to5m.ydaymed. * pg_24d,
             data = data.frame(Xtestproto))[, -1]
Ytest <- test %>%
  filtertrain5() %>%
  mutate(actual = box_cox(gpp / gpp.ydaymed, lambda = 0.1414141)) %>%
  as_tibble() %>%
  select(times, actual)


fcst_test <- ugarchforecast(garch_m1_autostart,
                           external.forecasts = list(mregfor = Xtest),
                           n.ahead = nrow(Xtest))

fcst_test_df <- tibble(mean = fitted(fcst_test)[, 1], sigma = sigma(fcst_test)[, 1]) %>%
  mutate(upper = mean + sigma, lower = mean - sigma)

fcst_test_df <- cbind(fcst_test_df, Ytest[1:nrow(fcst_test_df), ])

fcst_test_df %>%
  pivot_longer(cols = c("mean","actual")) %>%
  ggplot() +
  geom_ribbon(aes(x = times, ymin = lower, ymax = upper)) +
  geom_line(aes(x = times, y = value, col = name, lty = name))

plot(fcst_test, which = 1)
# plot(fcst_test, which = 3)
# plot(fcst_10ahead, which = 4)
```

The above predictions seem to to have features are roughly the correct time - this is good.
But still, magnitude is off quite a lot, and timing is also off.
Overall the variability of observed Y training is much larger than the forecasts, which indicates that something is wrong about the forecasts.
I suspect the above GARCH model is overfitting due to the very high correlation between successive time points.



### Garch Model m2: m1 with 1 sample every 24 days, and variance that depends on covariates
Subsampled to once every 24 days to reduce autocorrelation in the hope that fit would be better.
Use ARIMA order (2, 4) to match model "m6" in 1_6_exploration_pg_gpp_relation.Rmd, which is the best ARIMA model found so far.

```{r garch_m2}
filtertrain6 <- function(x){
  x <- x %>% filter(!is.na(pg_1to14m),
                    yday %in% seq(1, 366, by = 3 * 8), #reduce correlation in data fitting and reduce fitting time
                    !is.na(gpp))
  return(x)
}

Xproto <- model.frame(~ pg_1to5m * I(1 / pg_1to5m.ydaymed) * pg_24d,
             data = train %>% filtertrain6())
Xproto <- scale(Xproto)
Xscales <- attributes(Xproto)

X <- model.matrix(~ 
              pg_1to5m * I.1.pg_1to5m.ydaymed. * pg_24d,
             data = data.frame(Xproto))[, -1]  #doesn't fit when intercept is included!

Y <- train %>%
  filtertrain6() %>%
  mutate(y = box_cox(gpp / gpp.ydaymed, lambda = 0.1414141)) %>%
  as_tibble() %>%
  select(y) %>%
  as.matrix() %>%
  as.vector()

pacf(Y)
acf(Y)

garch_m2_spec <- ugarchspec(
  variance.model = list(model = "sGARCH", 
                        garchOrder = c(1, 1),
                        external.regressors = X),
  mean.model = list(armaOrder = c(2, 4),
                    external.regressors = X))

garch_m2 <- ugarchfit(garch_m2_spec, Y,
                      solver = "gosolnp",
                      solver.control = list(parallel = TRUE),
                      fit.control = list(stationarity = TRUE, scale = 1))
plot(garch_m2, which = "all")

Xtestproto <- model.frame(~ pg_1to5m * I(1 / pg_1to5m.ydaymed) * pg_24d,
             data = test %>% filtertrain6())
Xtestproto <- scale(Xtestproto,
                    center = attr(Xproto, "scaled:center"),
                    scale = attr(Xproto, "scaled:scale"))
Xtest <- model.matrix(~ 
              pg_1to5m * I.1.pg_1to5m.ydaymed. * pg_24d,
             data = data.frame(Xtestproto))[, -1]
Ytest <- test %>%
  filtertrain6() %>%
  mutate(actual = box_cox(gpp / gpp.ydaymed, lambda = 0.1414141)) %>%
  as_tibble() %>%
  select(times, actual)
fcst_test <- ugarchforecast(garch_m2,
                           external.forecasts = list(mregfor = Xtest, vregfor = Xtest),
                           n.ahead = nrow(Xtest))
fcst_test_df <- tibble(mean = fitted(fcst_test)[, 1], sigma = sigma(fcst_test)[, 1]) %>%
  mutate(upper = mean + sigma, lower = mean - sigma)

fcst_test_df <- cbind(fcst_test_df, Ytest[1:nrow(fcst_test_df), ])

fcst_test_df %>%
  pivot_longer(cols = c("mean","actual")) %>%
  ggplot() +
  geom_ribbon(aes(x = times, ymin = lower, ymax = upper)) +
  geom_line(aes(x = times, y = value, col = name, lty = name))
```

The above predictions seems nearly identical to the model fitted with higher frequency.
I am intrigued that the mean model fitted using essentially linear regression (model "m1" in 1_6_**.Rmd) does better than these GARCH models.
Could it be that it is still overfitting? __This suggests going back to a multisite model__

### Fourier Exploration of LIND1
```{r fourier_LIND1}
#fill NA values with a simple interplation (there is only one NA value in train)
missing_idx <- which(is.na(train$gpp))
train$gpp[missing_idx] <- mean(train$gpp[missing_idx + c(-1, 1)])

library(spectral)
gpp.k <- fft(train$gpp)
gpp.k2 <- spec.fft(train$gpp)
plot(Re(gpp.k[1:30]))
plot(gpp.k2)

plot(train$gpp, type = "l")
lines(Re(filter.fft(train$gpp, fc = 0, BW = 0.1)), col = "red")
```

I can't see much happening here?

### Look at Lag of LIND1 Standardised GPP Events vs Rainfall
#### When GPP / median.GPP >= k
Current draft event is GPP / median.GPP >= k, where k = 1 seems reasonable as it answers the question *when does GPP get back to normal levels after a dry spell?*

```{r LIND1eventslag_absolutevalue}
train %>%
  mutate(st_gpp = gpp / gpp.ydaymed) %>%
  mutate(aboveusual = (st_gpp >= 1)) %>%
  mutate(start_aboveusual = (1 == aboveusual - lag(aboveusual, 1))) %>%
  pivot_longer(cols = c("st_gpp", "pg_8d", "pg_24d")) %>%
  ggplot() +
  geom_hline(aes(yintercept = 1), col = "grey", lty = "dashed") +
  geom_line(aes(x = yday, y = value, col = name)) +
  geom_vline(aes(xintercept = yday), data = function(x) x %>% filter(start_aboveusual == TRUE)) +
  facet_wrap(vars(year)) +
  ylim(c(0, 15))
```

This actually looks pretty good for LIND1. I wonder if slope of standardised GPP would be a better representation though? 

A difficulty with both methods is that a very overgrazed land may not have a threshold until the grazing practise changes. 
In fact the method would be more about answering the question *when GPP has gone back to usual values, has it rained recently?*  There could be other factors, like reduced grazing and change in cropping, that cause it to go back to usual GPP.
If there are lots of returns to usual GPP *not explained* by rain, then can we make any interpretation at all?

__These methods won't capture *lack* of response to lots of rain.__

__What about fitting rainfall to slope of standardised GPP only when GPP is below usual?__
It wouldn't capture *delay* in GPP growth, but it would capture a slow GPP growth.
I've looked at this in a section below.

#### When GPP / median.GPP >= k and GPP has been consistently low

```{r LIND1eventslag_absolutevalue}
k = 1.05
q = 0.95
train %>%
  mutate(st_gpp = gpp / gpp.ydaymed) %>%
  mutate(aboveusual = (st_gpp >= k)) %>%
  mutate(belowusual = (st_gpp < q)) %>%
  mutate(start_aboveusual = (4 <= aboveusual +
                               lag(belowusual, 1) +
                               lag(belowusual, 2) +
                               lag(belowusual, 3))) %>%
  pivot_longer(cols = c("st_gpp", "pg_8d", "pg_24d")) %>%
  ggplot() +
  geom_hline(aes(yintercept = 1), col = "grey", lty = "dashed") +
  geom_line(aes(x = yday, y = value, col = name)) +
  geom_vline(aes(xintercept = yday), data = function(x) x %>% filter(start_aboveusual == TRUE)) +
  facet_wrap(vars(year)) +
  ylim(c(0, 15))
```

#### When smoothed(GPP / median.GPP) >= k
To reduce the number of GPP spike events, smooth the standardised GPP curve. 
```{r LIND1eventslag_absolutevalue_smoothed}
train %>%
  mutate(st_gpp = gpp / gpp.ydaymed) %>%
  mutate(st_gpp.smooth = RcppRoll::roll_mean(st_gpp, 4, fill = NA)) %>%
  mutate(aboveusual = (st_gpp.smooth >= 1)) %>%
  mutate(start_aboveusual = (1 == aboveusual - lag(aboveusual, 1))) %>%
  pivot_longer(cols = c("st_gpp", "st_gpp.smooth", "pg_8d", "pg_24d")) %>%
  ggplot() +
  geom_hline(aes(yintercept = 1), col = "grey", lty = "dashed") +
  geom_line(aes(x = yday, y = value, col = name)) +
  geom_vline(aes(xintercept = yday), data = function(x) x %>% filter(start_aboveusual == TRUE)) +
  facet_wrap(vars(year)) +
  ylim(c(0, 15))
```

In above 17 of the events marked are not coming from a spell of below usual GPP. The remainder, `r 31 - 17` do have low GPP spells before them.
This is an unacceptably high rate of false events to be fitting a model to.
Another difficulty is that there aren't many events for a farm, so unlikely to be able to describe overgrazing at given times - just a summary for the whole farm over time.
However, I like the part about this method that focuses only on the time points of interest, not on the whole time series.

This model also isn't quite a time-to-event model because there is no starting time.
Would this starting time be rain of a certain amount?
But these rain events are not all equal. Some are short sharp burst, some are sustained...

+ 24d past rainfall
+ GPP now (also summarises impact of long term past rainfall)
+ future rainfall (lots ==> faster and larger GPP return)

How about instead. Finding the GPP events and predicting how far back in time the rainfall was?

```{r LIND1eventslag_absolutevalue_smoothed_rainevents}
pg_24dthresh <- quantile(train$pg_24d, 0.75, na.rm = TRUE)
train %>%
  select(pg_24d) %>%
  ggplot() +
  geom_histogram(aes(x = pg_24d, y = ..density..)) +
  geom_vline(xintercept = pg_24dthresh, col = "red")
train %>%
  mutate(st_gpp = gpp / gpp.ydaymed) %>%
  mutate(st_gpp.smooth = RcppRoll::roll_meanr(st_gpp, 4, fill = NA)) %>%
  mutate(aboveusual = (st_gpp.smooth >= 0.9)) %>%
  mutate(start_aboveusual = (1 == aboveusual - lag(aboveusual, 1))) %>%
  mutate(newwet = (pg_24d > pg_24dthresh) & (st_gpp.smooth < 0.9)) %>%
  mutate(startnewwet = (1 == newwet - lag(newwet, 1))) %>%
  pivot_longer(cols = c("st_gpp", "st_gpp.smooth", "pg_8d", "pg_24d")) %>%
  ggplot() +
  geom_hline(aes(yintercept = 1), col = "grey", lty = "dashed") +
  geom_line(aes(x = yday, y = value, col = name)) +
  geom_vline(aes(xintercept = yday), data = function(x) x %>% filter(start_aboveusual == TRUE)) +
  geom_vline(aes(xintercept = yday), data = function(x) x %>% filter(startnewwet == TRUE),
             col = "red") +
  facet_wrap(vars(year)) +
  ylim(c(0, 15))
```


Each "response" is a time between two events, high pg_24d and usual GPP values.
Covariates of: previous rainfall, rainfall that occured between the two events, GPP at first event, yday of first event.


#### When slope of GPP / median.GPP >= k
```{r LIND1eventslag_slopethreshold}
train %>%
  mutate(st_gpp = gpp / gpp.ydaymed) %>%
  mutate(st_gpp.dot = 2* (st_gpp - lag(st_gpp, 1))) %>%
  pivot_longer(cols = c("st_gpp.dot", "st_gpp", "pg_8d", "pg_24d")) %>%
  ggplot() +
  geom_hline(aes(yintercept = 1), col = "grey", lty = "dashed") +
  geom_line(aes(x = yday, y = value, col = name)) +
  facet_wrap(vars(year)) +
  scale_colour_viridis_d(end = 0.8) +
  ylim(c(-10, 10))
```

At the end of 2010 a large amount of rain (high pg_24d) does not correspond to a positive slope in standardised GPP, because the amount of GPP is already above normal.


#### Slope of smoothed(GPP / median.GPP) predicted by rainfall when GPP below usual?
It wouldn't capture *delay* in GPP growth, but it would capture a slow GPP growth.
It is also mostly fitting zero rain fall to zero GPP.
Would I cut out times that don't have much rainfall?

There is an interaction between rainfall and current GPP when impacting slope in GPP.

*This is similar to stuff I have already done: it is a time series with differencing order of 1, and a covariate of the actual time series value.*

```{r LIND1eventslag_slope_conditional}
train %>%
  mutate(st_gpp = RcppRoll::roll_meanr(gpp / gpp.ydaymed, n = 4, fill = NA)) %>%
  mutate(st_gpp.dot = 10 * (st_gpp - lag(st_gpp, 1))) %>%
  mutate(st_gpp.dot = if_else(st_gpp <= 1, st_gpp.dot, as.numeric(NA))) %>%
  pivot_longer(cols = c("st_gpp.dot", "st_gpp", "pg_8d", "pg_24d")) %>%
  ggplot() +
  geom_hline(aes(yintercept = 1), col = "grey", lty = "dashed") +
  geom_line(aes(x = yday, y = value, col = name)) +
  facet_wrap(vars(year)) +
  scale_colour_viridis_d(end = 0.8) +
  ylim(c(-10, 10))
```
---
title: "LIND1 GPP vs PG fit"
author: "Kassel Hingee"
date: "03/04/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
out <- lapply(c("sf", "tsibble", 'lubridate', "viridis",
                'ggplot2', 'tidyr', 'grid', 'gridExtra', 
                'feasts', 'dplyr', 'gtable', 'fable',
                'mgcv'),
       library, character.only = TRUE)
out <- lapply(paste0("../functions/", list.files("../functions/")), source)
```

```{r preparedata}
#load data and convert to tsibbles
load("../private/data/remote_sensed/pg_daily.Rdata")
pg_daily$times <- as_date(pg_daily$times)
pg <- pg_daily %>%
  pivot_longer(-times, names_to = "site", values_to = "pg") %>%
  as_tsibble(key = site, index = times)
load("../private/data/remote_sensed/gpp_8d.Rdata")
gpp <- gpp_8d %>%
  pivot_longer(-times, names_to = "site", values_to = "gpp") %>%
  as_tsibble(key = site, index = times)
pggpp <- as_tsibble(dplyr::full_join(pg, gpp, by = c("times", "site")), key = site, index = times)

# add times breakdowns
pggpp <- pggpp %>%
  mutate(yday = yday(times),
         year = year(times))
  

#interpolate gpp
pggpp <- pggpp %>%
  group_by_key() %>% #key is site
  mutate(lininterp_gpp = zoo::na.approx(gpp, rule = 2, na.rm = FALSE)) %>% #rule 2 means edges are assigned last value
  ungroup()

#make sure sites are ordered alphabetically
pggpp <- pggpp %>%
  arrange(site) %>%
  mutate(site = factor(site, ordered = TRUE))

#separate alpha part of site code
pggpp <- pggpp %>%
  mutate(farm = factor(substr(site, 1, 4)),
         sitenum = factor(as.integer(substr(site, 5, 5))))


# Add cumulative rainfalls
pggpp <- pggpp %>% 
  group_by_key() %>%
  mutate(pg_cumsum = cumsum(pg)) %>%
  mutate(pg_1to7 = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 8), # 1 up to 8 days behind (excluding 8th day)
         pg_1to15 = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 16), # 1 to 16 days behind
         pg_1to1m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 31),
         pg_1to2m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 2*31),
         pg_1to3m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 3*31),
         pg_1to4m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 4*31),
         pg_1to5m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 5*31),
         pg_1to6m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 6*31),
         pg_1to7m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 7*31),
         pg_1to8m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 8*31),
         pg_1to10m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 10*31),
         pg_1to12m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 12*31),
         pg_1to14m = lag(pg_cumsum, n = 1) - lag(pg_cumsum, n = 14*31),
         ) %>%
  mutate(pg_8d = (pg_cumsum - lag(pg_cumsum, n = 8))/8.0,
         #0 to day 7 (8 days) cumulative rainfall to correspond with gaps in GPP
         pg_24d = (pg_cumsum - lag(pg_cumsum, n = 3*8)) / 24) %>% 
         # every 24 days (3*8) of GPP should be less correlated (given average GPP), this pg_24d corresponds to the rain through that period
  ungroup()
```

```{r seasonalreferences}
# simple median of values
ydaymedian <- pggpp %>%
  filter(yday %in% seq(1, 366, by = 8)) %>%
  group_by(site) %>%
  index_by(yday) %>%
  summarise(gpp.ydaymed = median(gpp),
            pg_8d.ydaymed = median(pg_8d),
            pg_24d.ydaymed = median(pg_24d),
            pg_1to5m.ydaymed = median(pg_1to5m, na.rm = TRUE))
pggpp <- left_join(pggpp, ydaymedian, by = c("site", "yday"))
```

```{r preparetraintest}
train <- pggpp %>%
  filter(yday %in% seq(1, 366, by = 8)) %>%
  filter(sitenum == 1) %>% #so not doubling up at farms
  filter_index(. ~ "2016-12-31") %>%
  dplyr::select(-pg_cumsum, -lininterp_gpp) %>%
  mutate(gpp = if_else(gpp < 0.1, as.double(NA), gpp)) #remove outlying GPP values that are super low

test <- pggpp %>%
  filter_index("2017-01-01" ~ .) %>%
  filter(sitenum == 1) %>% #so not doubling up at farms
  filter(yday %in% seq(1, 366, by = 8)) %>%
  dplyr::select(-pg_cumsum)
```


In the above very small GPP values have been removed. There was `r sum(is.na(train$gpp))` of them, which corresponds to `r sum(is.na(train$gpp)) / length(train$gpp)` of the data.

## Site LIND1
```{r select_pggpp_for_lind1}
train <- train %>%
  filter(site == "LIND1")
test <- test %>%
  filter(site == "LIND1")
```

```{r view_lind1}
train %>%
  mutate(st_gpp = gpp / gpp.ydaymed) %>%
  pivot_longer(cols = c("st_gpp", "pg_8d", "pg_24d")) %>%
  ggplot() +
  geom_line(aes(x = yday, y = value, col = name)) +
  facet_wrap(vars(year)) +
  ylim(c(0, 10))
```


### Attempt a garch model
`garch()` from `tseries` package does not appear to allow external predictors.
Similarly the `fGarch` package does not have any ability for external regressors.
However the `rugarch` package does.

```{r garch_m1}
library(rugarch)

filtertrain5 <- function(x){
  x <- x %>% filter(!is.na(pg_1to14m),
                    yday %in% seq(1, 366, by = 1 * 8), #reduce correlation in data fitting and reduce fitting time
                    !is.na(gpp))
  return(x)
}

Xproto <- model.frame(~ pg_1to5m * I(1 / pg_1to5m.ydaymed) * pg_24d,
             data = train %>% filtertrain5())
Xproto <- scale(Xproto)
Xscales <- attributes(Xproto)

X <- model.matrix(~ 
              pg_1to5m * I.1.pg_1to5m.ydaymed. * pg_24d,
             data = data.frame(Xproto))[, -1]  #doesn't fit when intercept is included!

Y <- train %>%
  filtertrain5() %>%
  mutate(y = box_cox(gpp / gpp.ydaymed, lambda = 0.1414141)) %>%
  as_tibble() %>%
  select(y) %>%
  as.matrix() %>%
  as.vector()

acfs <- acf(Y)
pacfs <- pacf(Y)

garch_m1_spec <- ugarchspec(variance.model = list(
  model = "sGARCH", garchOrder = c(1, 1)),
  mean.model = list(armaOrder = c(1, 1),
                    external.regressors = X),
  start.pars = list(mu = median(Y),
                    ar1 = pacfs$acf[1, 1, 1],
                    ma1 = acfs$acf[1, 1, 1],
                    alpha1 = 0.8,
                    beta1 = 0.8) )
 

garch_m1 <- ugarchfit(garch_m1_spec, Y,
                      # out.sample = 500,
                      solver = "hybrid",
                      solver.control = list(parallel = TRUE),
                      fit.control = list(stationarity = TRUE, scale = 1))

# print(garch_m1)
plot(garch_m1, which = "all")
infocriteria(garch_m1)
```

```{r fit_withoutstartparameters}
garch_m1_spec <- ugarchspec(variance.model = list(
  model = "sGARCH", garchOrder = c(1, 1)),
  mean.model = list(armaOrder = c(1, 1),
                    external.regressors = X))
 

garch_m1_autostart <- ugarchfit(garch_m1_spec, Y,
                      # out.sample = 500,
                      solver = "hybrid",
                      solver.control = list(parallel = TRUE),
                      fit.control = list(stationarity = TRUE, scale = 1))

# print(garch_m1_autostart)
plot(garch_m1_autostart, which = "all")
tibble(times = zoo::index(fitted(garch_m1_autostart)), fit = as.numeric(fitted(garch_m1_autostart)), actual = Y) %>%
  pivot_longer(cols = c("fit", "actual")) %>%
  ggplot() +
  geom_line(aes(x = times, y = value, col = name, lty = name))
infocriteria(garch_m1_autostart)
show(garch_m1_autostart)
```

The fitted model using automatically determined initial search values satisfies assumptions better, particularly the normal distribution of the residuals. It also has much better information criteria values. I will investigate its predictions in the following.

```{r garch_m1_autostart_prediction}
# fcst_10ahead_uncertainty <- ugarchboot(garch_m1_autostart,
#                            method = "partial", #uncertainy is only in model's randmoness, uncertainty of fitted parameters not included.
#                            n.ahead = 10)

Xtestproto <- model.frame(~ pg_1to5m * I(1 / pg_1to5m.ydaymed) * pg_24d,
             data = test %>% filtertrain5())
Xtestproto <- scale(Xtestproto,
                    center = attr(Xproto, "scaled:center"),
                    scale = attr(Xproto, "scaled:scale"))
Xtest <- model.matrix(~ 
              pg_1to5m * I.1.pg_1to5m.ydaymed. * pg_24d,
             data = data.frame(Xtestproto))[, -1]
Ytest <- test %>%
  filtertrain5() %>%
  mutate(actual = box_cox(gpp / gpp.ydaymed, lambda = 0.1414141)) %>%
  as_tibble() %>%
  select(times, actual)


fcst_test <- ugarchforecast(garch_m1_autostart,
                           external.forecasts = list(mregfor = Xtest),
                           n.ahead = nrow(Xtest))

fcst_test_df <- tibble(mean = fitted(fcst_test)[, 1], sigma = sigma(fcst_test)[, 1]) %>%
  mutate(upper = mean + sigma, lower = mean - sigma)

fcst_test_df <- cbind(fcst_test_df, Ytest[1:nrow(fcst_test_df), ])

fcst_test_df %>%
  pivot_longer(cols = c("mean","actual")) %>%
  ggplot() +
  geom_ribbon(aes(x = times, ymin = lower, ymax = upper)) +
  geom_line(aes(x = times, y = value, col = name, lty = name))

plot(fcst_test, which = 1)
# plot(fcst_test, which = 3)
# plot(fcst_10ahead, which = 4)
```

The above predictions seem to to have features are roughly the correct time - this is good.
But still, magnitude is off quite a lot, and timing is also off.
Overall the variability of observed Y training is much larger than the forecasts, which indicates that something is wrong about the forecasts.
I suspect the above GARCH model is overfitting due to the very high correlation between successive time points.



## Model m2: m1 with 1 sample per month (roughly)
Subsampled to once every 24 days to reduce autocorrelation in the hope that fit would be better.
Use ARIMA order (2, 4) to match model "m6" in 1_6_exploration_pg_gpp_relation.Rmd, which is the best ARIMA model found so far.

```{r garch_m2}
filtertrain6 <- function(x){
  x <- x %>% filter(!is.na(pg_1to14m),
                    yday %in% seq(1, 366, by = 3 * 8), #reduce correlation in data fitting and reduce fitting time
                    !is.na(gpp))
  return(x)
}

Xproto <- model.frame(~ pg_1to5m * I(1 / pg_1to5m.ydaymed) * pg_24d,
             data = train %>% filtertrain6())
Xproto <- scale(Xproto)
Xscales <- attributes(Xproto)

X <- model.matrix(~ 
              pg_1to5m * I.1.pg_1to5m.ydaymed. * pg_24d,
             data = data.frame(Xproto))[, -1]  #doesn't fit when intercept is included!

Y <- train %>%
  filtertrain6() %>%
  mutate(y = box_cox(gpp / gpp.ydaymed, lambda = 0.1414141)) %>%
  as_tibble() %>%
  select(y) %>%
  as.matrix() %>%
  as.vector()

pacf(Y)
acf(Y)

garch_m2_spec <- ugarchspec(
  variance.model = list(model = "sGARCH", 
                        garchOrder = c(1, 1),
                        external.regressors = X),
  mean.model = list(armaOrder = c(2, 4),
                    external.regressors = X))

garch_m2 <- ugarchfit(garch_m2_spec, Y,
                      solver = "gosolnp",
                      solver.control = list(parallel = TRUE),
                      fit.control = list(stationarity = TRUE, scale = 1))
plot(garch_m2, which = "all")

Xtestproto <- model.frame(~ pg_1to5m * I(1 / pg_1to5m.ydaymed) * pg_24d,
             data = test %>% filtertrain6())
Xtestproto <- scale(Xtestproto,
                    center = attr(Xproto, "scaled:center"),
                    scale = attr(Xproto, "scaled:scale"))
Xtest <- model.matrix(~ 
              pg_1to5m * I.1.pg_1to5m.ydaymed. * pg_24d,
             data = data.frame(Xtestproto))[, -1]
Ytest <- test %>%
  filtertrain6() %>%
  mutate(actual = box_cox(gpp / gpp.ydaymed, lambda = 0.1414141)) %>%
  as_tibble() %>%
  select(times, actual)
fcst_test <- ugarchforecast(garch_m2,
                           external.forecasts = list(mregfor = Xtest, vregfor = Xtest),
                           n.ahead = nrow(Xtest))
fcst_test_df <- tibble(mean = fitted(fcst_test)[, 1], sigma = sigma(fcst_test)[, 1]) %>%
  mutate(upper = mean + sigma, lower = mean - sigma)

fcst_test_df <- cbind(fcst_test_df, Ytest[1:nrow(fcst_test_df), ])

fcst_test_df %>%
  pivot_longer(cols = c("mean","actual")) %>%
  ggplot() +
  geom_ribbon(aes(x = times, ymin = lower, ymax = upper)) +
  geom_line(aes(x = times, y = value, col = name, lty = name))
```

The above predictions seems nearly identical to the model fitted with higher frequency.
I am intrigued that the mean model fitted using essentially linear regression (model "m1" in 1_6_**.Rmd) does better than these GARCH models.
Could it be that it is still overfitting? __This suggests going back to a multisite model__

## Fourier Exploration of LIND1
```{r fourier_LIND1}
#fill NA values with a simple interplation (there is only one NA value in train)
missing_idx <- which(is.na(train$gpp))
train$gpp[missing_idx] <- mean(train$gpp[missing_idx + c(-1, 1)])

library(spectral)
gpp.k <- fft(train$gpp)
gpp.k2 <- spec.fft(train$gpp)
plot(Re(gpp.k[1:30]))
plot(gpp.k2)

plot(train$gpp, type = "l")
lines(Re(filter.fft(train$gpp, fc = 0, BW = 0.1)), col = "red")
```
